{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1VfdgyVFhcPG3ESWwdOCdOllEHnPliO3c","timestamp":1702858812053},{"file_id":"1SMqKVBXkPyqquhQLch_-Pb-FPA9W2scG","timestamp":1675719602048}],"gpuType":"V100"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Assignment 5\n","## Note: DON'T RUN THIS UNTIL YOU HAVE MADE SURE YOUR CODE WORKS LOCALLY; THIS NOTEBOOK TAKES **~2-3 HOUR TO RUN**.\n","\n","## See https://edstem.org/us/courses/33056/discussion/2523780 For a guide to use Colab GPUs.\n","\n","Please make a copy of this Colab notebook in your own Google Drive to edit it.\n","\n","This serves as an Azure alternative in case we have a dreadful case of GPU shortage. Feel free to copy this setup for other assignments."],"metadata":{"id":"hyX-uxPIo7Qa"}},{"cell_type":"markdown","source":["## 1. Upload your files"],"metadata":{"id":"Dxsy3M-YpFWA"}},{"cell_type":"markdown","source":["### Option 1: Upload your files to your Google Drive, and mount the drive\n","You can upload your files to your own Google drive. This is the ''safest'' way to set up the Colab, since your files are stored in your Google Drive instead of the Colab Runtime, which may terminate if you leave it on for too long (~8 hours, or if you put your laptop to sleep mode) and lose your files."],"metadata":{"id":"uE0rGVDlq_2b"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"6Z0FgJnDrNpz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1703002222768,"user_tz":-60,"elapsed":35256,"user":{"displayName":"Fida Fida","userId":"16516579540060451036"}},"outputId":"01e43485-98d5-404e-9687-47b30dc7d22b"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["### Option 2: Zip your folder, upload, then unzip\n","Zip your code folder. Upload the folder of your code to the runtime (drag and drop) so that the structure of Files would look like the following:\n","- ..\n","- sample_data\n","- your_folder.zip\n","\n","Then run the following code block to unzip the file.\n","\n","NOTE THAT IF YOU LEAVE YOUR COLAB NOTEBOOK ALONE FOR TOO LONG, THERE IS A RISK THAT YOU WILL LOSE YOUR FILES."],"metadata":{"id":"rSqOgenpGFG-"}},{"cell_type":"code","source":["! unzip solution.zip # replace this with the name of your zip file"],"metadata":{"id":"jAbGRHZIEFhl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2. Relocate to your code directory"],"metadata":{"id":"d9AVj8wKEYDD"}},{"cell_type":"code","source":["import argparse"],"metadata":{"id":"LEktZ5E0WUVi","executionInfo":{"status":"ok","timestamp":1702859747735,"user_tz":-60,"elapsed":309,"user":{"displayName":"Fida Fida","userId":"16516579540060451036"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/CS224N/a5_code\n","! ls # verify that you are in the right directory"],"metadata":{"id":"CLJeBmWgEcxP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1703002223839,"user_tz":-60,"elapsed":1094,"user":{"displayName":"Fida Fida","userId":"16516579540060451036"}},"outputId":"ef36be6e-fcbf-4007-c357-3eab22fc220d"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/CS224N/a5_code\n","birth_dev.tsv\t\texpt\t\t  src\t\t\t\t       wiki.txt\n","birth_places_train.tsv\tlocal_env_a5.yml  vanilla.model.params\n","birth_test_inputs.tsv\tmingpt-demo\t  vanilla.nopretrain.dev.predictions\n","collect_submission.sh\tscripts\t\t  vanilla.nopretrain.test.predictions\n"]}]},{"cell_type":"markdown","source":["## 3. Run assignment-specific commands"],"metadata":{"id":"9AXw039SE0Ml"}},{"cell_type":"markdown","source":["To run on GPU, please make sure your runtime type is using the GPU accelerator! To do so, click on:\n","\n","![colab1.PNG](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAIYAAAAzCAYAAABSfnBXAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAAFiUAABYlAUlSJPAAAARCSURBVHhe7ZzPahtXFIf9CH2EvkHfYVYDhgEXrQxxBYGQxdBFRMCiUGPIEEg2VgI2XdSG4EXQwqgbFYwMRllEJUalBEFxQoOwQDQgYphgKir49R7NlTQzuh5J6aQal98HB2aO/ix8P5977r2DVkCIAYpBjFAMYoRiECMUgxihGMQIxSBGKAYxQjGIkZXz83MwGPFgxSBGKAYxQjGIEYpBjFAMYmQ5YnQrcC0L1ihWcygcNOEP9OtDfNQ2LTg/tPT9iCZK6jPObjyv0N/rHnV14npOTk5uRCyLJYpRUkOsuXyDH10bWye+Tih6VRTWHDhre2hFhAnEsGwP9Sud0rSf52Hb9txi7O/vRyJrUAxF98iFtTPJ9H4uYP1ZU1UNB3uvdXKIiOHBe2jDe9HXOaGFvTUXpR2XYqRARitGFxVXCaFmC/+4CDskTCBGCY3Xe3DuV9HTWej7xrN1ipEC2egxrBy2jt6orkLTLiMvU4hcX9ZQjEwbQcWo+1IhCqgOzeijritIc2f+HoNiXE8GKkZQHUpnk2mhrf7rJ01nD9X7FrzT0esihouKGnvpKfLP28BVHd5qIA/FSIdMTCX9Fx5st6IUEaQShKuJjs2arigTMYbfc+sQDdWPjFYpFCMdPosY7/76U19dQ7zHGLRxeFs3mdIrxFciskKxRtNGSIxhNZGVSx5lVTgEipEOqYvxzdsn+OLVLfz68Q+dMRAXQyGrEEs1jy93nWB6iBD0EMGAh8XQ1eZ2GaNPLCLGTYhlkaoYIsXKy6/x1W/38OHvjzpLbiKpiUEp/l+kIgalyBadTgfb29uJIe9J4l+LQSmyycXFBTY2NqZWd5KT12YxUwwZ+Pzbp/ouCqXINnE55pVCSBRDBlsGXQY/Lsd/JsWVj37kEI0swkiORaQQZlYMkxxpSiHLy3Gpk+P33QZ6YxFkn8LG1mno1HUK2TmdLF/nxbQ0zGKkgQixiBTCXD1GWI4vm3dTrRSRfYd+F7WHOdgP6pNzk5l8uhjxDa6sbXKlJcanMHfzGZYjLSmEqQ2pgWyJR3cyS2fBdffYQ35VqouN/IMausPKEhajj+bTHHI7jdhDP9NQjGTmFkMQGWQaSUsKwbRTOcz9FByoj8V4X4G79hiNS3U96KL6nY3isdSViRj+WQnr31a0MMlQjGQWEuNzcK0YOjcWY3j87uKw1Ys1o4EY5V+UOOslNOecgyhGMtkVI14xFP7vVTy+p3qQqanEQe77LRTzqqJQjFTInhgJPcaYQQ/1R46WR8S4g3JHiXPqIfdI9RfBuxKhGMlkSwzDqmQkRvDMRhlteV5n4KMREWPUfPZQ27ThJS5vAyhGMpkQY7KPkUfxILyPEa4YPloHBeQSVyWKThmuPAo4ww35o9+EWBZLF4NkE4pBjFAMYoRiECMUgxihGMQIf4OLYQxWDGKEYhAjFIMYoRjECMUgBoB/AFmUbgFoGPesAAAAAElFTkSuQmCC)\n","\n","on the upper-right hand corner, then click \"Change runtime type\" in the bottom-left of the popped-up panel, then select \"GPU\" as your hardware accelerator."],"metadata":{"id":"FMlxFOX7Hskt"}},{"cell_type":"markdown","source":["Now you can directly run the commands specified by the assignment files in the following code blocks."],"metadata":{"id":"0HS7OfiZFZ9f"}},{"cell_type":"markdown","source":["Q 2d) Make predictions (without pretraining).\n","\n"],"metadata":{"id":"4t9flXje7qhm"}},{"cell_type":"code","source":["! bash scripts/run_vanilla_no_pretraining.sh"],"metadata":{"id":"XtafSjURwW0a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1703003718851,"user_tz":-60,"elapsed":201251,"user":{"displayName":"Fida Fida","userId":"16516579540060451036"}},"outputId":"748d9eea-2514-4b35-a64e-9a808231479e"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-12-19 16:31:59.275709: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2023-12-19 16:31:59.275765: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2023-12-19 16:31:59.277142: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2023-12-19 16:31:59.284790: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-12-19 16:32:00.405460: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","data has 418352 characters, 256 unique.\n","number of parameters: 3323392\n","epoch 1 iter 7: train loss 3.41666. lr 5.999844e-04: 100% 8/8 [00:02<00:00,  3.48it/s]\n","epoch 2 iter 7: train loss 2.70247. lr 5.999351e-04: 100% 8/8 [00:00<00:00,  8.05it/s]\n","epoch 3 iter 7: train loss 2.38465. lr 5.998521e-04: 100% 8/8 [00:00<00:00,  8.49it/s]\n","epoch 4 iter 7: train loss 2.20060. lr 5.997352e-04: 100% 8/8 [00:00<00:00,  8.13it/s]\n","epoch 5 iter 7: train loss 2.08889. lr 5.995847e-04: 100% 8/8 [00:00<00:00,  9.24it/s]\n","epoch 6 iter 7: train loss 2.02022. lr 5.994004e-04: 100% 8/8 [00:00<00:00,  9.11it/s]\n","epoch 7 iter 7: train loss 1.98461. lr 5.991823e-04: 100% 8/8 [00:00<00:00,  9.21it/s]\n","epoch 8 iter 7: train loss 1.91993. lr 5.989306e-04: 100% 8/8 [00:00<00:00,  9.04it/s]\n","epoch 9 iter 7: train loss 1.85944. lr 5.986453e-04: 100% 8/8 [00:00<00:00,  9.23it/s]\n","epoch 10 iter 7: train loss 1.81430. lr 5.983263e-04: 100% 8/8 [00:00<00:00,  9.17it/s]\n","epoch 11 iter 7: train loss 1.75555. lr 5.979737e-04: 100% 8/8 [00:00<00:00,  9.17it/s]\n","epoch 12 iter 7: train loss 1.69777. lr 5.975876e-04: 100% 8/8 [00:00<00:00,  8.89it/s]\n","epoch 13 iter 7: train loss 1.64675. lr 5.971680e-04: 100% 8/8 [00:00<00:00,  9.19it/s]\n","epoch 14 iter 7: train loss 1.56704. lr 5.967149e-04: 100% 8/8 [00:00<00:00,  9.17it/s]\n","epoch 15 iter 7: train loss 1.47862. lr 5.962284e-04: 100% 8/8 [00:00<00:00,  8.68it/s]\n","epoch 16 iter 7: train loss 1.43470. lr 5.957086e-04: 100% 8/8 [00:00<00:00,  8.34it/s]\n","epoch 17 iter 7: train loss 1.32977. lr 5.951554e-04: 100% 8/8 [00:00<00:00,  8.02it/s]\n","epoch 18 iter 7: train loss 1.26865. lr 5.945690e-04: 100% 8/8 [00:00<00:00,  8.58it/s]\n","epoch 19 iter 7: train loss 1.19863. lr 5.939495e-04: 100% 8/8 [00:00<00:00,  8.30it/s]\n","epoch 20 iter 7: train loss 1.15823. lr 5.932969e-04: 100% 8/8 [00:00<00:00,  9.37it/s]\n","epoch 21 iter 7: train loss 1.08159. lr 5.926112e-04: 100% 8/8 [00:00<00:00,  9.35it/s]\n","epoch 22 iter 7: train loss 1.05700. lr 5.918926e-04: 100% 8/8 [00:00<00:00,  8.99it/s]\n","epoch 23 iter 7: train loss 0.97882. lr 5.911412e-04: 100% 8/8 [00:00<00:00,  9.06it/s]\n","epoch 24 iter 7: train loss 0.94169. lr 5.903569e-04: 100% 8/8 [00:00<00:00,  9.13it/s]\n","epoch 25 iter 7: train loss 0.89933. lr 5.895400e-04: 100% 8/8 [00:00<00:00,  9.18it/s]\n","epoch 26 iter 7: train loss 0.90790. lr 5.886905e-04: 100% 8/8 [00:00<00:00,  9.03it/s]\n","epoch 27 iter 7: train loss 0.84106. lr 5.878084e-04: 100% 8/8 [00:00<00:00,  9.20it/s]\n","epoch 28 iter 7: train loss 0.84064. lr 5.868940e-04: 100% 8/8 [00:00<00:00,  9.12it/s]\n","epoch 29 iter 7: train loss 0.81476. lr 5.859473e-04: 100% 8/8 [00:00<00:00,  9.02it/s]\n","epoch 30 iter 7: train loss 0.77991. lr 5.849683e-04: 100% 8/8 [00:00<00:00,  8.44it/s]\n","epoch 31 iter 7: train loss 0.76609. lr 5.839573e-04: 100% 8/8 [00:00<00:00,  8.30it/s]\n","epoch 32 iter 7: train loss 0.71615. lr 5.829143e-04: 100% 8/8 [00:00<00:00,  8.57it/s]\n","epoch 33 iter 7: train loss 0.73904. lr 5.818395e-04: 100% 8/8 [00:00<00:00,  8.07it/s]\n","epoch 34 iter 7: train loss 0.71392. lr 5.807329e-04: 100% 8/8 [00:00<00:00,  8.18it/s]\n","epoch 35 iter 7: train loss 0.66582. lr 5.795947e-04: 100% 8/8 [00:00<00:00,  9.14it/s]\n","epoch 36 iter 7: train loss 0.67679. lr 5.784251e-04: 100% 8/8 [00:00<00:00,  9.06it/s]\n","epoch 37 iter 7: train loss 0.63829. lr 5.772241e-04: 100% 8/8 [00:00<00:00,  9.02it/s]\n","epoch 38 iter 7: train loss 0.62754. lr 5.759918e-04: 100% 8/8 [00:00<00:00,  9.11it/s]\n","epoch 39 iter 7: train loss 0.60875. lr 5.747285e-04: 100% 8/8 [00:00<00:00,  9.17it/s]\n","epoch 40 iter 7: train loss 0.59521. lr 5.734343e-04: 100% 8/8 [00:00<00:00,  9.08it/s]\n","epoch 41 iter 7: train loss 0.59778. lr 5.721093e-04: 100% 8/8 [00:00<00:00,  9.14it/s]\n","epoch 42 iter 7: train loss 0.55757. lr 5.707537e-04: 100% 8/8 [00:00<00:00,  9.17it/s]\n","epoch 43 iter 7: train loss 0.56264. lr 5.693675e-04: 100% 8/8 [00:00<00:00,  9.38it/s]\n","epoch 44 iter 7: train loss 0.53887. lr 5.679511e-04: 100% 8/8 [00:00<00:00,  9.08it/s]\n","epoch 45 iter 7: train loss 0.56341. lr 5.665044e-04: 100% 8/8 [00:00<00:00,  8.44it/s]\n","epoch 46 iter 7: train loss 0.55055. lr 5.650278e-04: 100% 8/8 [00:01<00:00,  7.67it/s]\n","epoch 47 iter 7: train loss 0.51711. lr 5.635213e-04: 100% 8/8 [00:00<00:00,  8.06it/s]\n","epoch 48 iter 7: train loss 0.50631. lr 5.619852e-04: 100% 8/8 [00:00<00:00,  8.39it/s]\n","epoch 49 iter 7: train loss 0.49201. lr 5.604195e-04: 100% 8/8 [00:00<00:00,  8.54it/s]\n","epoch 50 iter 7: train loss 0.50870. lr 5.588246e-04: 100% 8/8 [00:00<00:00,  9.35it/s]\n","epoch 51 iter 7: train loss 0.46770. lr 5.572005e-04: 100% 8/8 [00:00<00:00,  9.14it/s]\n","epoch 52 iter 7: train loss 0.45482. lr 5.555474e-04: 100% 8/8 [00:00<00:00,  9.12it/s]\n","epoch 53 iter 7: train loss 0.42691. lr 5.538656e-04: 100% 8/8 [00:00<00:00,  9.29it/s]\n","epoch 54 iter 7: train loss 0.46048. lr 5.521552e-04: 100% 8/8 [00:00<00:00,  9.16it/s]\n","epoch 55 iter 7: train loss 0.42070. lr 5.504164e-04: 100% 8/8 [00:00<00:00,  9.28it/s]\n","epoch 56 iter 7: train loss 0.41048. lr 5.486494e-04: 100% 8/8 [00:00<00:00,  9.18it/s]\n","epoch 57 iter 7: train loss 0.39820. lr 5.468544e-04: 100% 8/8 [00:00<00:00,  9.12it/s]\n","epoch 58 iter 7: train loss 0.40099. lr 5.450316e-04: 100% 8/8 [00:00<00:00,  9.19it/s]\n","epoch 59 iter 7: train loss 0.37513. lr 5.431812e-04: 100% 8/8 [00:00<00:00,  9.37it/s]\n","epoch 60 iter 7: train loss 0.35287. lr 5.413034e-04: 100% 8/8 [00:00<00:00,  8.40it/s]\n","epoch 61 iter 7: train loss 0.36470. lr 5.393985e-04: 100% 8/8 [00:00<00:00,  8.10it/s]\n","epoch 62 iter 7: train loss 0.33615. lr 5.374666e-04: 100% 8/8 [00:00<00:00,  8.32it/s]\n","epoch 63 iter 7: train loss 0.32553. lr 5.355080e-04: 100% 8/8 [00:00<00:00,  8.42it/s]\n","epoch 64 iter 7: train loss 0.31865. lr 5.335229e-04: 100% 8/8 [00:00<00:00,  8.31it/s]\n","epoch 65 iter 7: train loss 0.30295. lr 5.315115e-04: 100% 8/8 [00:00<00:00,  8.74it/s]\n","epoch 66 iter 7: train loss 0.28931. lr 5.294740e-04: 100% 8/8 [00:00<00:00,  9.08it/s]\n","epoch 67 iter 7: train loss 0.27590. lr 5.274107e-04: 100% 8/8 [00:00<00:00,  9.04it/s]\n","epoch 68 iter 7: train loss 0.29432. lr 5.253217e-04: 100% 8/8 [00:00<00:00,  9.13it/s]\n","epoch 69 iter 7: train loss 0.29394. lr 5.232074e-04: 100% 8/8 [00:00<00:00,  9.17it/s]\n","epoch 70 iter 7: train loss 0.26575. lr 5.210680e-04: 100% 8/8 [00:00<00:00,  9.13it/s]\n","epoch 71 iter 7: train loss 0.24321. lr 5.189037e-04: 100% 8/8 [00:00<00:00,  9.25it/s]\n","epoch 72 iter 7: train loss 0.24061. lr 5.167147e-04: 100% 8/8 [00:00<00:00,  9.26it/s]\n","epoch 73 iter 7: train loss 0.20322. lr 5.145014e-04: 100% 8/8 [00:00<00:00,  9.25it/s]\n","epoch 74 iter 7: train loss 0.19651. lr 5.122639e-04: 100% 8/8 [00:00<00:00,  9.46it/s]\n","epoch 75 iter 7: train loss 0.21611. lr 5.100024e-04: 100% 8/8 [00:01<00:00,  7.93it/s]\n","2023-12-19 16:33:27.644076: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2023-12-19 16:33:27.644141: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2023-12-19 16:33:27.645608: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2023-12-19 16:33:27.652952: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-12-19 16:33:28.753038: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","data has 418352 characters, 256 unique.\n","number of parameters: 3323392\n","500it [00:53,  9.39it/s]\n","Correct: 2.0 out of 500.0: 0.4%\n","2023-12-19 16:34:27.826708: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2023-12-19 16:34:27.826767: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2023-12-19 16:34:27.828157: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2023-12-19 16:34:27.835733: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-12-19 16:34:28.931918: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","data has 418352 characters, 256 unique.\n","number of parameters: 3323392\n","437it [00:45,  9.57it/s]\n","No gold birth places provided; returning (0,0)\n","Predictions written to vanilla.nopretrain.test.predictions; no targets provided\n"]}]},{"cell_type":"markdown","source":["Q 2f) Pretrain, finetune, and make predictions.\n","\n"],"metadata":{"id":"KvosRNlqwyIh"}},{"cell_type":"code","source":["! bash scripts/run_vanilla.sh"],"metadata":{"id":"HICuFxL8Bnvl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1703005551967,"user_tz":-60,"elapsed":566590,"user":{"displayName":"Fida Fida","userId":"16516579540060451036"}},"outputId":"2c9a7fac-96ec-4c06-e45a-b1d5ce10b5d3"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-12-19 16:45:30.168518: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2023-12-19 16:45:30.168578: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2023-12-19 16:45:30.169996: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2023-12-19 16:45:30.179192: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-12-19 16:45:31.288432: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","data has 418352 characters, 256 unique.\n","number of parameters: 3323392\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","epoch 1 iter 22: train loss 3.44349. lr 3.102347e-03: 100% 23/23 [00:02<00:00, 10.26it/s]\n","epoch 2 iter 22: train loss 3.43442. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 16.95it/s]\n","epoch 3 iter 22: train loss 3.20944. lr 2.953074e-03: 100% 23/23 [00:01<00:00, 17.26it/s]\n","epoch 4 iter 22: train loss 2.89576. lr 5.999939e-03: 100% 23/23 [00:01<00:00, 15.97it/s]\n","epoch 5 iter 22: train loss 2.83131. lr 2.991488e-03: 100% 23/23 [00:01<00:00, 14.12it/s]\n","epoch 6 iter 22: train loss 2.80578. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.06it/s]\n","epoch 7 iter 22: train loss 2.77257. lr 3.063946e-03: 100% 23/23 [00:01<00:00, 14.93it/s]\n","epoch 8 iter 22: train loss 2.75077. lr 5.998600e-03: 100% 23/23 [00:01<00:00, 16.78it/s]\n","epoch 9 iter 22: train loss 2.68613. lr 2.880641e-03: 100% 23/23 [00:01<00:00, 16.89it/s]\n","epoch 10 iter 22: train loss 2.67894. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 17.25it/s]\n","epoch 11 iter 22: train loss 2.65243. lr 3.174730e-03: 100% 23/23 [00:01<00:00, 17.02it/s]\n","epoch 12 iter 22: train loss 2.63757. lr 5.993165e-03: 100% 23/23 [00:01<00:00, 17.39it/s]\n","epoch 13 iter 22: train loss 2.61663. lr 2.769957e-03: 100% 23/23 [00:01<00:00, 17.04it/s]\n","epoch 14 iter 22: train loss 2.57654. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 15.14it/s]\n","epoch 15 iter 22: train loss 2.56248. lr 3.285276e-03: 100% 23/23 [00:01<00:00, 13.94it/s]\n","epoch 16 iter 22: train loss 2.57555. lr 5.983642e-03: 100% 23/23 [00:01<00:00, 14.45it/s]\n","epoch 17 iter 22: train loss 2.53422. lr 2.659588e-03: 100% 23/23 [00:02<00:00, 11.15it/s]\n","epoch 18 iter 22: train loss 2.49625. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 16.37it/s]\n","epoch 19 iter 22: train loss 2.49693. lr 3.395432e-03: 100% 23/23 [00:01<00:00, 16.87it/s]\n","epoch 20 iter 22: train loss 2.52475. lr 5.970044e-03: 100% 23/23 [00:01<00:00, 16.68it/s]\n","epoch 21 iter 22: train loss 2.48503. lr 2.549683e-03: 100% 23/23 [00:01<00:00, 17.02it/s]\n","epoch 22 iter 22: train loss 2.44692. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 17.15it/s]\n","epoch 23 iter 22: train loss 2.45244. lr 3.505048e-03: 100% 23/23 [00:01<00:00, 16.95it/s]\n","epoch 24 iter 22: train loss 2.47320. lr 5.952389e-03: 100% 23/23 [00:01<00:00, 14.75it/s]\n","epoch 25 iter 22: train loss 2.41799. lr 2.440393e-03: 100% 23/23 [00:01<00:00, 14.17it/s]\n","epoch 26 iter 22: train loss 2.38100. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 13.99it/s]\n","epoch 27 iter 22: train loss 2.38431. lr 3.613975e-03: 100% 23/23 [00:01<00:00, 14.72it/s]\n","epoch 28 iter 22: train loss 2.41152. lr 5.930702e-03: 100% 23/23 [00:01<00:00, 16.80it/s]\n","epoch 29 iter 22: train loss 2.38191. lr 2.331867e-03: 100% 23/23 [00:01<00:00, 16.96it/s]\n","epoch 30 iter 22: train loss 2.30391. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 16.85it/s]\n","epoch 31 iter 22: train loss 2.33026. lr 3.722062e-03: 100% 23/23 [00:01<00:00, 17.47it/s]\n","epoch 32 iter 22: train loss 2.33962. lr 5.905012e-03: 100% 23/23 [00:01<00:00, 17.04it/s]\n","epoch 33 iter 22: train loss 2.31175. lr 2.224255e-03: 100% 23/23 [00:01<00:00, 17.35it/s]\n","epoch 34 iter 22: train loss 2.22874. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 15.53it/s]\n","epoch 35 iter 22: train loss 2.26444. lr 3.829164e-03: 100% 23/23 [00:01<00:00, 14.55it/s]\n","epoch 36 iter 22: train loss 2.26475. lr 5.875354e-03: 100% 23/23 [00:01<00:00, 11.62it/s]\n","epoch 37 iter 22: train loss 2.20770. lr 2.117701e-03: 100% 23/23 [00:01<00:00, 14.53it/s]\n","epoch 38 iter 22: train loss 2.18773. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 17.16it/s]\n","epoch 39 iter 22: train loss 2.19325. lr 3.935133e-03: 100% 23/23 [00:01<00:00, 16.86it/s]\n","epoch 40 iter 22: train loss 2.18602. lr 5.841769e-03: 100% 23/23 [00:01<00:00, 17.08it/s]\n","epoch 41 iter 22: train loss 2.15033. lr 2.012353e-03: 100% 23/23 [00:01<00:00, 16.96it/s]\n","epoch 42 iter 22: train loss 2.06289. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 16.72it/s]\n","epoch 43 iter 22: train loss 2.11063. lr 4.039824e-03: 100% 23/23 [00:01<00:00, 17.24it/s]\n","epoch 44 iter 22: train loss 2.13167. lr 5.804302e-03: 100% 23/23 [00:01<00:00, 15.20it/s]\n","epoch 45 iter 22: train loss 2.07287. lr 1.908354e-03: 100% 23/23 [00:01<00:00, 14.10it/s]\n","epoch 46 iter 22: train loss 2.05372. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 13.97it/s]\n","epoch 47 iter 22: train loss 2.09632. lr 4.143096e-03: 100% 23/23 [00:01<00:00, 14.33it/s]\n","epoch 48 iter 22: train loss 2.07419. lr 5.763005e-03: 100% 23/23 [00:01<00:00, 17.06it/s]\n","epoch 49 iter 22: train loss 2.00740. lr 1.805846e-03: 100% 23/23 [00:01<00:00, 16.93it/s]\n","epoch 50 iter 22: train loss 1.92604. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 17.02it/s]\n","epoch 51 iter 22: train loss 1.99678. lr 4.244806e-03: 100% 23/23 [00:01<00:00, 17.08it/s]\n","epoch 52 iter 22: train loss 1.97919. lr 5.717935e-03: 100% 23/23 [00:01<00:00, 16.94it/s]\n","epoch 53 iter 22: train loss 1.91682. lr 1.704968e-03: 100% 23/23 [00:01<00:00, 17.18it/s]\n","epoch 54 iter 22: train loss 1.85903. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 15.72it/s]\n","epoch 55 iter 22: train loss 1.88362. lr 4.344816e-03: 100% 23/23 [00:01<00:00, 13.32it/s]\n","epoch 56 iter 22: train loss 1.86183. lr 5.669152e-03: 100% 23/23 [00:01<00:00, 12.46it/s]\n","epoch 57 iter 22: train loss 1.83810. lr 1.605860e-03: 100% 23/23 [00:01<00:00, 14.55it/s]\n","epoch 58 iter 22: train loss 1.76193. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 16.96it/s]\n","epoch 59 iter 22: train loss 1.81468. lr 4.442989e-03: 100% 23/23 [00:01<00:00, 16.99it/s]\n","epoch 60 iter 22: train loss 1.81728. lr 5.616723e-03: 100% 23/23 [00:01<00:00, 16.59it/s]\n","epoch 61 iter 22: train loss 1.72365. lr 1.508656e-03: 100% 23/23 [00:01<00:00, 16.66it/s]\n","epoch 62 iter 22: train loss 1.69589. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 16.81it/s]\n","epoch 63 iter 22: train loss 1.71723. lr 4.539191e-03: 100% 23/23 [00:01<00:00, 16.48it/s]\n","epoch 64 iter 22: train loss 1.74596. lr 5.560720e-03: 100% 23/23 [00:01<00:00, 14.19it/s]\n","epoch 65 iter 22: train loss 1.64128. lr 1.413488e-03: 100% 23/23 [00:01<00:00, 14.61it/s]\n","epoch 66 iter 22: train loss 1.57243. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.11it/s]\n","epoch 67 iter 22: train loss 1.62351. lr 4.633291e-03: 100% 23/23 [00:01<00:00, 14.93it/s]\n","epoch 68 iter 22: train loss 1.65119. lr 5.501220e-03: 100% 23/23 [00:01<00:00, 16.80it/s]\n","epoch 69 iter 22: train loss 1.56631. lr 1.320488e-03: 100% 23/23 [00:01<00:00, 16.86it/s]\n","epoch 70 iter 22: train loss 1.49924. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 16.85it/s]\n","epoch 71 iter 22: train loss 1.52843. lr 4.725160e-03: 100% 23/23 [00:01<00:00, 16.62it/s]\n","epoch 72 iter 22: train loss 1.59332. lr 5.438303e-03: 100% 23/23 [00:01<00:00, 16.66it/s]\n","epoch 73 iter 22: train loss 1.50432. lr 1.229782e-03: 100% 23/23 [00:01<00:00, 16.25it/s]\n","epoch 74 iter 22: train loss 1.46822. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.06it/s]\n","epoch 75 iter 22: train loss 1.48831. lr 4.814672e-03: 100% 23/23 [00:01<00:00, 12.58it/s]\n","epoch 76 iter 22: train loss 1.52137. lr 5.372056e-03: 100% 23/23 [00:01<00:00, 14.32it/s]\n","epoch 77 iter 22: train loss 1.43639. lr 1.141493e-03: 100% 23/23 [00:01<00:00, 15.59it/s]\n","epoch 78 iter 22: train loss 1.38220. lr 6.622880e-04: 100% 23/23 [00:01<00:00, 16.91it/s]\n","epoch 79 iter 22: train loss 1.42461. lr 4.901706e-03: 100% 23/23 [00:01<00:00, 17.11it/s]\n","epoch 80 iter 22: train loss 1.51069. lr 5.302569e-03: 100% 23/23 [00:01<00:00, 16.96it/s]\n","epoch 81 iter 22: train loss 1.40407. lr 1.055743e-03: 100% 23/23 [00:01<00:00, 16.66it/s]\n","epoch 82 iter 22: train loss 1.36445. lr 7.333594e-04: 100% 23/23 [00:01<00:00, 16.93it/s]\n","epoch 83 iter 22: train loss 1.39849. lr 4.986143e-03: 100% 23/23 [00:01<00:00, 16.74it/s]\n","epoch 84 iter 22: train loss 1.40278. lr 5.229938e-03: 100% 23/23 [00:01<00:00, 14.43it/s]\n","epoch 85 iter 22: train loss 1.38006. lr 9.726487e-04: 100% 23/23 [00:01<00:00, 13.96it/s]\n","epoch 86 iter 22: train loss 1.30633. lr 8.075268e-04: 100% 23/23 [00:01<00:00, 14.30it/s]\n","epoch 87 iter 22: train loss 1.37208. lr 5.067867e-03: 100% 23/23 [00:01<00:00, 15.09it/s]\n","epoch 88 iter 22: train loss 1.39476. lr 5.154260e-03: 100% 23/23 [00:01<00:00, 17.16it/s]\n","epoch 89 iter 22: train loss 1.29597. lr 8.923233e-04: 100% 23/23 [00:01<00:00, 17.08it/s]\n","epoch 90 iter 22: train loss 1.25028. lr 8.846887e-04: 100% 23/23 [00:01<00:00, 17.07it/s]\n","epoch 91 iter 22: train loss 1.30005. lr 5.146767e-03: 100% 23/23 [00:01<00:00, 16.94it/s]\n","epoch 92 iter 22: train loss 1.37341. lr 5.075640e-03: 100% 23/23 [00:01<00:00, 16.95it/s]\n","epoch 93 iter 22: train loss 1.29577. lr 8.148766e-04: 100% 23/23 [00:01<00:00, 16.56it/s]\n","epoch 94 iter 22: train loss 1.22776. lr 9.647399e-04: 100% 23/23 [00:02<00:00, 11.07it/s]\n","epoch 95 iter 22: train loss 1.28764. lr 5.222734e-03: 100% 23/23 [00:01<00:00, 14.34it/s]\n","epoch 96 iter 22: train loss 1.29784. lr 4.994185e-03: 100% 23/23 [00:01<00:00, 14.12it/s]\n","epoch 97 iter 22: train loss 1.26504. lr 7.404145e-04: 100% 23/23 [00:01<00:00, 15.72it/s]\n","epoch 98 iter 22: train loss 1.18127. lr 1.047571e-03: 100% 23/23 [00:01<00:00, 17.43it/s]\n","epoch 99 iter 22: train loss 1.26120. lr 5.295665e-03: 100% 23/23 [00:01<00:00, 16.42it/s]\n","epoch 100 iter 22: train loss 1.32287. lr 4.910006e-03: 100% 23/23 [00:01<00:00, 16.89it/s]\n","epoch 101 iter 22: train loss 1.24248. lr 6.690387e-04: 100% 23/23 [00:01<00:00, 16.89it/s]\n","epoch 102 iter 22: train loss 1.15096. lr 1.133069e-03: 100% 23/23 [00:01<00:00, 17.31it/s]\n","epoch 103 iter 22: train loss 1.25959. lr 5.365461e-03: 100% 23/23 [00:01<00:00, 16.40it/s]\n","epoch 104 iter 22: train loss 1.29058. lr 4.823219e-03: 100% 23/23 [00:01<00:00, 15.10it/s]\n","epoch 105 iter 22: train loss 1.14797. lr 6.008466e-04: 100% 23/23 [00:01<00:00, 14.36it/s]\n","epoch 106 iter 22: train loss 1.13372. lr 1.221116e-03: 100% 23/23 [00:01<00:00, 14.21it/s]\n","epoch 107 iter 22: train loss 1.22907. lr 5.432026e-03: 100% 23/23 [00:01<00:00, 15.50it/s]\n","epoch 108 iter 22: train loss 1.26002. lr 4.733941e-03: 100% 23/23 [00:01<00:00, 16.76it/s]\n","epoch 109 iter 22: train loss 1.18228. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 17.15it/s]\n","epoch 110 iter 22: train loss 1.13796. lr 1.311594e-03: 100% 23/23 [00:01<00:00, 16.98it/s]\n","epoch 111 iter 22: train loss 1.20857. lr 5.495270e-03: 100% 23/23 [00:01<00:00, 17.40it/s]\n","epoch 112 iter 22: train loss 1.22212. lr 4.642295e-03: 100% 23/23 [00:01<00:00, 16.66it/s]\n","epoch 113 iter 22: train loss 1.15605. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 15.35it/s]\n","epoch 114 iter 22: train loss 1.08331. lr 1.404377e-03: 100% 23/23 [00:01<00:00, 13.72it/s]\n","epoch 115 iter 22: train loss 1.20047. lr 5.555105e-03: 100% 23/23 [00:01<00:00, 14.71it/s]\n","epoch 116 iter 22: train loss 1.20769. lr 4.548406e-03: 100% 23/23 [00:01<00:00, 14.14it/s]\n","epoch 117 iter 22: train loss 1.11166. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 16.47it/s]\n","epoch 118 iter 22: train loss 1.10482. lr 1.499340e-03: 100% 23/23 [00:01<00:00, 16.70it/s]\n","epoch 119 iter 22: train loss 1.18092. lr 5.611450e-03: 100% 23/23 [00:01<00:00, 15.59it/s]\n","epoch 120 iter 22: train loss 1.15147. lr 4.452402e-03: 100% 23/23 [00:01<00:00, 17.14it/s]\n","epoch 121 iter 22: train loss 1.10545. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 16.85it/s]\n","epoch 122 iter 22: train loss 1.05738. lr 1.596352e-03: 100% 23/23 [00:01<00:00, 16.98it/s]\n","epoch 123 iter 22: train loss 1.13540. lr 5.664228e-03: 100% 23/23 [00:01<00:00, 16.75it/s]\n","epoch 124 iter 22: train loss 1.14671. lr 4.354414e-03: 100% 23/23 [00:01<00:00, 14.47it/s]\n","epoch 125 iter 22: train loss 1.08628. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.71it/s]\n","epoch 126 iter 22: train loss 1.05847. lr 1.695282e-03: 100% 23/23 [00:01<00:00, 14.48it/s]\n","epoch 127 iter 22: train loss 1.08931. lr 5.713368e-03: 100% 23/23 [00:01<00:00, 15.94it/s]\n","epoch 128 iter 22: train loss 1.16312. lr 4.254576e-03: 100% 23/23 [00:01<00:00, 16.74it/s]\n","epoch 129 iter 22: train loss 1.07796. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 17.17it/s]\n","epoch 130 iter 22: train loss 1.03716. lr 1.795994e-03: 100% 23/23 [00:01<00:00, 17.03it/s]\n","epoch 131 iter 22: train loss 1.12499. lr 5.758801e-03: 100% 23/23 [00:01<00:00, 16.84it/s]\n","epoch 132 iter 22: train loss 1.13057. lr 4.153025e-03: 100% 23/23 [00:01<00:00, 15.43it/s]\n","epoch 133 iter 22: train loss 1.01348. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 16.32it/s]\n","epoch 134 iter 22: train loss 1.01065. lr 1.898350e-03: 100% 23/23 [00:01<00:00, 14.21it/s]\n","epoch 135 iter 22: train loss 1.06457. lr 5.800466e-03: 100% 23/23 [00:01<00:00, 14.08it/s]\n","epoch 136 iter 22: train loss 1.12100. lr 4.049899e-03: 100% 23/23 [00:01<00:00, 14.12it/s]\n","epoch 137 iter 22: train loss 1.02388. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 17.36it/s]\n","epoch 138 iter 22: train loss 1.03055. lr 2.002211e-03: 100% 23/23 [00:01<00:00, 16.95it/s]\n","epoch 139 iter 22: train loss 1.07606. lr 5.838306e-03: 100% 23/23 [00:01<00:00, 16.92it/s]\n","epoch 140 iter 22: train loss 1.07813. lr 3.945338e-03: 100% 23/23 [00:01<00:00, 16.83it/s]\n","epoch 141 iter 22: train loss 0.98590. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 16.98it/s]\n","epoch 142 iter 22: train loss 0.98409. lr 2.107435e-03: 100% 23/23 [00:01<00:00, 17.29it/s]\n","epoch 143 iter 22: train loss 1.09888. lr 5.872270e-03: 100% 23/23 [00:01<00:00, 16.22it/s]\n","epoch 144 iter 22: train loss 1.09356. lr 3.839487e-03: 100% 23/23 [00:01<00:00, 15.16it/s]\n","epoch 145 iter 22: train loss 0.98345. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.24it/s]\n","epoch 146 iter 22: train loss 0.93597. lr 2.213878e-03: 100% 23/23 [00:01<00:00, 14.29it/s]\n","epoch 147 iter 22: train loss 1.07801. lr 5.902310e-03: 100% 23/23 [00:01<00:00, 17.08it/s]\n","epoch 148 iter 22: train loss 1.05386. lr 3.732489e-03: 100% 23/23 [00:01<00:00, 17.35it/s]\n","epoch 149 iter 22: train loss 1.00355. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 17.00it/s]\n","epoch 150 iter 22: train loss 0.95295. lr 2.321394e-03: 100% 23/23 [00:01<00:00, 16.99it/s]\n","epoch 151 iter 22: train loss 1.04953. lr 5.928387e-03: 100% 23/23 [00:01<00:00, 16.64it/s]\n","epoch 152 iter 22: train loss 1.10016. lr 3.624491e-03: 100% 23/23 [00:01<00:00, 15.55it/s]\n","epoch 153 iter 22: train loss 0.95909. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 15.72it/s]\n","epoch 154 iter 22: train loss 0.97101. lr 2.429838e-03: 100% 23/23 [00:01<00:00, 14.04it/s]\n","epoch 155 iter 22: train loss 1.06761. lr 5.950463e-03: 100% 23/23 [00:01<00:00, 15.03it/s]\n","epoch 156 iter 22: train loss 1.04667. lr 3.515639e-03: 100% 23/23 [00:01<00:00, 14.26it/s]\n","epoch 157 iter 22: train loss 0.94443. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 17.13it/s]\n","epoch 158 iter 22: train loss 0.91633. lr 2.539060e-03: 100% 23/23 [00:01<00:00, 17.01it/s]\n","epoch 159 iter 22: train loss 1.02684. lr 5.968510e-03: 100% 23/23 [00:01<00:00, 17.18it/s]\n","epoch 160 iter 22: train loss 1.01590. lr 3.406083e-03: 100% 23/23 [00:01<00:00, 17.18it/s]\n","epoch 161 iter 22: train loss 0.96037. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 16.97it/s]\n","epoch 162 iter 22: train loss 0.90533. lr 2.648912e-03: 100% 23/23 [00:01<00:00, 17.05it/s]\n","epoch 163 iter 22: train loss 1.02207. lr 5.982502e-03: 100% 23/23 [00:01<00:00, 15.98it/s]\n","epoch 164 iter 22: train loss 1.02100. lr 3.295973e-03: 100% 23/23 [00:01<00:00, 14.01it/s]\n","epoch 165 iter 22: train loss 0.91274. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.63it/s]\n","epoch 166 iter 22: train loss 0.90206. lr 2.759243e-03: 100% 23/23 [00:01<00:00, 14.81it/s]\n","epoch 167 iter 22: train loss 1.02126. lr 5.992421e-03: 100% 23/23 [00:01<00:00, 16.90it/s]\n","epoch 168 iter 22: train loss 1.00475. lr 3.185458e-03: 100% 23/23 [00:01<00:00, 17.08it/s]\n","epoch 169 iter 22: train loss 0.89178. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 16.87it/s]\n","epoch 170 iter 22: train loss 0.88751. lr 2.869903e-03: 100% 23/23 [00:01<00:00, 16.84it/s]\n","epoch 171 iter 22: train loss 1.00173. lr 5.998252e-03: 100% 23/23 [00:01<00:00, 14.40it/s]\n","epoch 172 iter 22: train loss 0.98806. lr 3.074690e-03: 100% 23/23 [00:01<00:00, 16.90it/s]\n","epoch 173 iter 22: train loss 0.89642. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 15.21it/s]\n","epoch 174 iter 22: train loss 0.89713. lr 2.980741e-03: 100% 23/23 [00:01<00:00, 14.84it/s]\n","epoch 175 iter 22: train loss 1.01835. lr 5.999988e-03: 100% 23/23 [00:01<00:00, 13.84it/s]\n","epoch 176 iter 22: train loss 0.99009. lr 2.963821e-03: 100% 23/23 [00:01<00:00, 14.59it/s]\n","epoch 177 iter 22: train loss 0.88749. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 17.21it/s]\n","epoch 178 iter 22: train loss 0.89642. lr 3.091605e-03: 100% 23/23 [00:01<00:00, 17.04it/s]\n","epoch 179 iter 22: train loss 1.01381. lr 5.997627e-03: 100% 23/23 [00:01<00:00, 17.09it/s]\n","epoch 180 iter 22: train loss 0.97170. lr 2.853000e-03: 100% 23/23 [00:01<00:00, 17.16it/s]\n","epoch 181 iter 22: train loss 0.90280. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 17.12it/s]\n","epoch 182 iter 22: train loss 0.87792. lr 3.202344e-03: 100% 23/23 [00:01<00:00, 16.96it/s]\n","epoch 183 iter 22: train loss 0.99789. lr 5.991171e-03: 100% 23/23 [00:01<00:00, 15.37it/s]\n","epoch 184 iter 22: train loss 1.00366. lr 2.742380e-03: 100% 23/23 [00:01<00:00, 13.88it/s]\n","epoch 185 iter 22: train loss 0.86779. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.56it/s]\n","epoch 186 iter 22: train loss 0.90685. lr 3.312807e-03: 100% 23/23 [00:01<00:00, 14.89it/s]\n","epoch 187 iter 22: train loss 0.99889. lr 5.980630e-03: 100% 23/23 [00:01<00:00, 17.20it/s]\n","epoch 188 iter 22: train loss 0.90935. lr 2.632112e-03: 100% 23/23 [00:01<00:00, 16.86it/s]\n","epoch 189 iter 22: train loss 0.83524. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 16.66it/s]\n","epoch 190 iter 22: train loss 0.86189. lr 3.422843e-03: 100% 23/23 [00:01<00:00, 16.95it/s]\n","epoch 191 iter 22: train loss 0.97225. lr 5.966017e-03: 100% 23/23 [00:01<00:00, 15.25it/s]\n","epoch 192 iter 22: train loss 0.96725. lr 2.522347e-03: 100% 23/23 [00:01<00:00, 17.02it/s]\n","epoch 193 iter 22: train loss 0.88614. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 15.28it/s]\n","epoch 194 iter 22: train loss 0.86508. lr 3.532300e-03: 100% 23/23 [00:01<00:00, 14.42it/s]\n","epoch 195 iter 22: train loss 0.97688. lr 5.947354e-03: 100% 23/23 [00:01<00:00, 14.23it/s]\n","epoch 196 iter 22: train loss 0.93601. lr 2.413234e-03: 100% 23/23 [00:01<00:00, 14.64it/s]\n","epoch 197 iter 22: train loss 0.85296. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 16.89it/s]\n","epoch 198 iter 22: train loss 0.88388. lr 3.641031e-03: 100% 23/23 [00:01<00:00, 16.95it/s]\n","epoch 199 iter 22: train loss 0.96967. lr 5.924665e-03: 100% 23/23 [00:01<00:00, 16.85it/s]\n","epoch 200 iter 22: train loss 0.95374. lr 2.304922e-03: 100% 23/23 [00:01<00:00, 17.17it/s]\n","epoch 201 iter 22: train loss 0.83849. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 16.93it/s]\n","epoch 202 iter 22: train loss 0.84173. lr 3.748887e-03: 100% 23/23 [00:01<00:00, 16.64it/s]\n","epoch 203 iter 22: train loss 0.97732. lr 5.897981e-03: 100% 23/23 [00:01<00:00, 14.92it/s]\n","epoch 204 iter 22: train loss 0.93818. lr 2.197560e-03: 100% 23/23 [00:01<00:00, 14.00it/s]\n","epoch 205 iter 22: train loss 0.82229. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.35it/s]\n","epoch 206 iter 22: train loss 0.82554. lr 3.855719e-03: 100% 23/23 [00:01<00:00, 14.89it/s]\n","epoch 207 iter 22: train loss 0.96106. lr 5.867339e-03: 100% 23/23 [00:01<00:00, 17.24it/s]\n","epoch 208 iter 22: train loss 0.89693. lr 2.091294e-03: 100% 23/23 [00:01<00:00, 17.08it/s]\n","epoch 209 iter 22: train loss 0.82446. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 17.06it/s]\n","epoch 210 iter 22: train loss 0.85457. lr 3.961383e-03: 100% 23/23 [00:01<00:00, 14.41it/s]\n","epoch 211 iter 22: train loss 0.94111. lr 5.832781e-03: 100% 23/23 [00:01<00:00, 16.05it/s]\n","epoch 212 iter 22: train loss 0.87747. lr 1.986269e-03: 100% 23/23 [00:01<00:00, 13.45it/s]\n","epoch 213 iter 22: train loss 0.85356. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 12.92it/s]\n","epoch 214 iter 22: train loss 0.84763. lr 4.065733e-03: 100% 23/23 [00:01<00:00, 12.86it/s]\n","epoch 215 iter 22: train loss 0.94123. lr 5.794354e-03: 100% 23/23 [00:01<00:00, 13.23it/s]\n","epoch 216 iter 22: train loss 0.90077. lr 1.882629e-03: 100% 23/23 [00:01<00:00, 13.34it/s]\n","epoch 217 iter 22: train loss 0.84414. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 15.44it/s]\n","epoch 218 iter 22: train loss 0.83306. lr 4.168628e-03: 100% 23/23 [00:01<00:00, 16.93it/s]\n","epoch 219 iter 22: train loss 0.93010. lr 5.752109e-03: 100% 23/23 [00:01<00:00, 16.91it/s]\n","epoch 220 iter 22: train loss 0.86569. lr 1.780514e-03: 100% 23/23 [00:01<00:00, 16.99it/s]\n","epoch 221 iter 22: train loss 0.82564. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 16.80it/s]\n","epoch 222 iter 22: train loss 0.80079. lr 4.269927e-03: 100% 23/23 [00:01<00:00, 16.80it/s]\n","epoch 223 iter 22: train loss 0.91787. lr 5.706106e-03: 100% 23/23 [00:01<00:00, 16.91it/s]\n","epoch 224 iter 22: train loss 0.86877. lr 1.680066e-03: 100% 23/23 [00:01<00:00, 14.65it/s]\n","epoch 225 iter 22: train loss 0.79248. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.31it/s]\n","epoch 226 iter 22: train loss 0.85660. lr 4.369491e-03: 100% 23/23 [00:01<00:00, 13.76it/s]\n","epoch 227 iter 22: train loss 0.90787. lr 5.656407e-03: 100% 23/23 [00:01<00:00, 14.90it/s]\n","epoch 228 iter 22: train loss 0.86544. lr 1.581420e-03: 100% 23/23 [00:01<00:00, 15.39it/s]\n","epoch 229 iter 22: train loss 0.82166. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 16.77it/s]\n","epoch 230 iter 22: train loss 0.84344. lr 4.467185e-03: 100% 23/23 [00:01<00:00, 17.19it/s]\n","epoch 231 iter 22: train loss 0.90881. lr 5.603080e-03: 100% 23/23 [00:01<00:00, 16.94it/s]\n","epoch 232 iter 22: train loss 0.84593. lr 1.484712e-03: 100% 23/23 [00:01<00:00, 17.06it/s]\n","epoch 233 iter 22: train loss 0.80737. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 16.61it/s]\n","epoch 234 iter 22: train loss 0.80606. lr 4.562874e-03: 100% 23/23 [00:01<00:00, 14.40it/s]\n","epoch 235 iter 22: train loss 0.92685. lr 5.546197e-03: 100% 23/23 [00:01<00:00, 13.85it/s]\n","epoch 236 iter 22: train loss 0.84197. lr 1.390073e-03: 100% 23/23 [00:01<00:00, 14.13it/s]\n","epoch 237 iter 22: train loss 0.79996. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 16.06it/s]\n","epoch 238 iter 22: train loss 0.80821. lr 4.656430e-03: 100% 23/23 [00:01<00:00, 16.88it/s]\n","epoch 239 iter 22: train loss 0.88812. lr 5.485836e-03: 100% 23/23 [00:01<00:00, 16.44it/s]\n","epoch 240 iter 22: train loss 0.83886. lr 1.297633e-03: 100% 23/23 [00:01<00:00, 16.87it/s]\n","epoch 241 iter 22: train loss 0.77549. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 16.93it/s]\n","epoch 242 iter 22: train loss 0.78097. lr 4.747722e-03: 100% 23/23 [00:01<00:00, 17.01it/s]\n","epoch 243 iter 22: train loss 0.93112. lr 5.422080e-03: 100% 23/23 [00:01<00:00, 16.53it/s]\n","epoch 244 iter 22: train loss 0.81439. lr 1.207519e-03: 100% 23/23 [00:01<00:00, 14.12it/s]\n","epoch 245 iter 22: train loss 0.78661. lr 6.110439e-04: 100% 23/23 [00:01<00:00, 14.05it/s]\n","epoch 246 iter 22: train loss 0.83967. lr 4.836628e-03: 100% 23/23 [00:01<00:00, 14.24it/s]\n","epoch 247 iter 22: train loss 0.89858. lr 5.355016e-03: 100% 23/23 [00:01<00:00, 15.86it/s]\n","epoch 248 iter 22: train loss 0.83539. lr 1.119853e-03: 100% 23/23 [00:01<00:00, 15.70it/s]\n","epoch 249 iter 22: train loss 0.75592. lr 6.797280e-04: 100% 23/23 [00:01<00:00, 16.56it/s]\n","epoch 250 iter 22: train loss 0.80192. lr 4.923025e-03: 100% 23/23 [00:01<00:00, 17.16it/s]\n","epoch 251 iter 22: train loss 0.88134. lr 5.284735e-03: 100% 23/23 [00:01<00:00, 16.97it/s]\n","epoch 252 iter 22: train loss 0.84534. lr 1.034755e-03: 100% 23/23 [00:01<00:00, 16.90it/s]\n","epoch 253 iter 22: train loss 0.76126. lr 7.515813e-04: 100% 23/23 [00:01<00:00, 16.51it/s]\n","epoch 254 iter 22: train loss 0.86239. lr 5.006795e-03: 100% 23/23 [00:01<00:00, 13.79it/s]\n","epoch 255 iter 22: train loss 0.89078. lr 5.211334e-03: 100% 23/23 [00:01<00:00, 14.74it/s]\n","epoch 256 iter 22: train loss 0.81635. lr 9.523406e-04: 100% 23/23 [00:01<00:00, 14.70it/s]\n","epoch 257 iter 22: train loss 0.77745. lr 8.265056e-04: 100% 23/23 [00:01<00:00, 16.74it/s]\n","epoch 258 iter 22: train loss 0.83460. lr 5.087825e-03: 100% 23/23 [00:01<00:00, 16.98it/s]\n","epoch 259 iter 22: train loss 0.88127. lr 5.134913e-03: 100% 23/23 [00:01<00:00, 16.89it/s]\n","epoch 260 iter 22: train loss 0.79109. lr 8.727234e-04: 100% 23/23 [00:01<00:00, 16.73it/s]\n","epoch 261 iter 22: train loss 0.76831. lr 9.043985e-04: 100% 23/23 [00:01<00:00, 16.69it/s]\n","epoch 262 iter 22: train loss 0.81119. lr 5.166002e-03: 100% 23/23 [00:01<00:00, 16.93it/s]\n","epoch 263 iter 22: train loss 0.89759. lr 5.055575e-03: 100% 23/23 [00:01<00:00, 16.19it/s]\n","epoch 264 iter 22: train loss 0.78768. lr 7.960117e-04: 100% 23/23 [00:01<00:00, 14.72it/s]\n","epoch 265 iter 22: train loss 0.76907. lr 9.851537e-04: 100% 23/23 [00:01<00:00, 14.30it/s]\n","epoch 266 iter 22: train loss 0.84151. lr 5.241222e-03: 100% 23/23 [00:01<00:00, 13.90it/s]\n","epoch 267 iter 22: train loss 0.89758. lr 4.973430e-03: 100% 23/23 [00:01<00:00, 14.41it/s]\n","epoch 268 iter 22: train loss 0.76025. lr 7.223104e-04: 100% 23/23 [00:01<00:00, 16.94it/s]\n","epoch 269 iter 22: train loss 0.76571. lr 1.068661e-03: 100% 23/23 [00:01<00:00, 17.17it/s]\n","epoch 270 iter 22: train loss 0.80616. lr 5.313380e-03: 100% 23/23 [00:01<00:00, 16.72it/s]\n","epoch 271 iter 22: train loss 0.87870. lr 4.888589e-03: 100% 23/23 [00:01<00:00, 17.04it/s]\n","epoch 272 iter 22: train loss 0.78533. lr 6.517201e-04: 100% 23/23 [00:01<00:00, 16.90it/s]\n","epoch 273 iter 22: train loss 0.72358. lr 1.154806e-03: 100% 23/23 [00:01<00:00, 16.26it/s]\n","epoch 274 iter 22: train loss 0.80095. lr 5.382378e-03: 100% 23/23 [00:01<00:00, 15.00it/s]\n","epoch 275 iter 22: train loss 0.88441. lr 4.801169e-03: 100% 23/23 [00:01<00:00, 13.82it/s]\n","epoch 276 iter 22: train loss 0.78674. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.21it/s]\n","epoch 277 iter 22: train loss 0.70504. lr 1.243471e-03: 100% 23/23 [00:01<00:00, 16.78it/s]\n","epoch 278 iter 22: train loss 0.75925. lr 5.448123e-03: 100% 23/23 [00:01<00:00, 16.89it/s]\n","epoch 279 iter 22: train loss 0.83857. lr 4.711289e-03: 100% 23/23 [00:01<00:00, 16.72it/s]\n","epoch 280 iter 22: train loss 0.78129. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 16.80it/s]\n","epoch 281 iter 22: train loss 0.73241. lr 1.334536e-03: 100% 23/23 [00:01<00:00, 16.65it/s]\n","epoch 282 iter 22: train loss 0.81219. lr 5.510523e-03: 100% 23/23 [00:01<00:00, 16.63it/s]\n","epoch 283 iter 22: train loss 0.84096. lr 4.619071e-03: 100% 23/23 [00:01<00:00, 16.34it/s]\n","epoch 284 iter 22: train loss 0.76227. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.94it/s]\n","epoch 285 iter 22: train loss 0.71716. lr 1.427875e-03: 100% 23/23 [00:01<00:00, 14.28it/s]\n","epoch 286 iter 22: train loss 0.80419. lr 5.569495e-03: 100% 23/23 [00:01<00:00, 14.43it/s]\n","epoch 287 iter 22: train loss 0.86516. lr 4.524642e-03: 100% 23/23 [00:01<00:00, 17.20it/s]\n","epoch 288 iter 22: train loss 0.75054. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 16.96it/s]\n","epoch 289 iter 22: train loss 0.73844. lr 1.523362e-03: 100% 23/23 [00:01<00:00, 17.07it/s]\n","epoch 290 iter 22: train loss 0.78574. lr 5.624957e-03: 100% 23/23 [00:01<00:00, 17.05it/s]\n","epoch 291 iter 22: train loss 0.88368. lr 4.428130e-03: 100% 23/23 [00:01<00:00, 16.53it/s]\n","epoch 292 iter 22: train loss 0.74731. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 17.03it/s]\n","epoch 293 iter 22: train loss 0.71536. lr 1.620865e-03: 100% 23/23 [00:01<00:00, 16.83it/s]\n","epoch 294 iter 22: train loss 0.78601. lr 5.676834e-03: 100% 23/23 [00:01<00:00, 13.88it/s]\n","epoch 295 iter 22: train loss 0.89061. lr 4.329668e-03: 100% 23/23 [00:01<00:00, 14.39it/s]\n","epoch 296 iter 22: train loss 0.76081. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.93it/s]\n","epoch 297 iter 22: train loss 0.68407. lr 1.720252e-03: 100% 23/23 [00:01<00:00, 16.25it/s]\n","epoch 298 iter 22: train loss 0.78001. lr 5.725055e-03: 100% 23/23 [00:01<00:00, 17.12it/s]\n","epoch 299 iter 22: train loss 0.84168. lr 4.229390e-03: 100% 23/23 [00:01<00:00, 17.16it/s]\n","epoch 300 iter 22: train loss 0.71165. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 16.57it/s]\n","epoch 301 iter 22: train loss 0.72892. lr 1.821388e-03: 100% 23/23 [00:01<00:00, 16.31it/s]\n","epoch 302 iter 22: train loss 0.80262. lr 5.769553e-03: 100% 23/23 [00:01<00:00, 17.38it/s]\n","epoch 303 iter 22: train loss 0.86937. lr 4.127433e-03: 100% 23/23 [00:01<00:00, 16.79it/s]\n","epoch 304 iter 22: train loss 0.76903. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 13.72it/s]\n","epoch 305 iter 22: train loss 0.66622. lr 1.924132e-03: 100% 23/23 [00:01<00:00, 11.95it/s]\n","epoch 306 iter 22: train loss 0.81422. lr 5.810269e-03: 100% 23/23 [00:01<00:00, 14.39it/s]\n","epoch 307 iter 22: train loss 0.83814. lr 4.023935e-03: 100% 23/23 [00:01<00:00, 16.76it/s]\n","epoch 308 iter 22: train loss 0.73114. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 16.83it/s]\n","epoch 309 iter 22: train loss 0.68834. lr 2.028347e-03: 100% 23/23 [00:01<00:00, 17.25it/s]\n","epoch 310 iter 22: train loss 0.82592. lr 5.847147e-03: 100% 23/23 [00:01<00:00, 16.99it/s]\n","epoch 311 iter 22: train loss 0.80397. lr 3.919039e-03: 100% 23/23 [00:01<00:00, 17.07it/s]\n","epoch 312 iter 22: train loss 0.72033. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 17.16it/s]\n","epoch 313 iter 22: train loss 0.69546. lr 2.133888e-03: 100% 23/23 [00:01<00:00, 16.80it/s]\n","epoch 314 iter 22: train loss 0.81948. lr 5.880135e-03: 100% 23/23 [00:01<00:00, 14.31it/s]\n","epoch 315 iter 22: train loss 0.80366. lr 3.812888e-03: 100% 23/23 [00:01<00:00, 14.32it/s]\n","epoch 316 iter 22: train loss 0.71153. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.35it/s]\n","epoch 317 iter 22: train loss 0.70102. lr 2.240613e-03: 100% 23/23 [00:01<00:00, 16.19it/s]\n","epoch 318 iter 22: train loss 0.78173. lr 5.909190e-03: 100% 23/23 [00:01<00:00, 17.26it/s]\n","epoch 319 iter 22: train loss 0.80451. lr 3.705627e-03: 100% 23/23 [00:01<00:00, 16.57it/s]\n","epoch 320 iter 22: train loss 0.73003. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 16.78it/s]\n","epoch 321 iter 22: train loss 0.71219. lr 2.348374e-03: 100% 23/23 [00:01<00:00, 16.76it/s]\n","epoch 322 iter 22: train loss 0.79700. lr 5.934272e-03: 100% 23/23 [00:01<00:00, 17.16it/s]\n","epoch 323 iter 22: train loss 0.81916. lr 3.597402e-03: 100% 23/23 [00:01<00:00, 16.59it/s]\n","epoch 324 iter 22: train loss 0.70402. lr 6.000000e-04: 100% 23/23 [00:02<00:00, 11.14it/s]\n","epoch 325 iter 22: train loss 0.64068. lr 2.457026e-03: 100% 23/23 [00:01<00:00, 14.90it/s]\n","epoch 326 iter 22: train loss 0.78721. lr 5.955345e-03: 100% 23/23 [00:01<00:00, 14.26it/s]\n","epoch 327 iter 22: train loss 0.78894. lr 3.488361e-03: 100% 23/23 [00:01<00:00, 17.11it/s]\n","epoch 328 iter 22: train loss 0.70979. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 17.39it/s]\n","epoch 329 iter 22: train loss 0.67499. lr 2.566420e-03: 100% 23/23 [00:01<00:00, 16.74it/s]\n","epoch 330 iter 22: train loss 0.80409. lr 5.972382e-03: 100% 23/23 [00:01<00:00, 17.08it/s]\n","epoch 331 iter 22: train loss 0.78678. lr 3.378652e-03: 100% 23/23 [00:01<00:00, 16.85it/s]\n","epoch 332 iter 22: train loss 0.69628. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 17.01it/s]\n","epoch 333 iter 22: train loss 0.65744. lr 2.676405e-03: 100% 23/23 [00:01<00:00, 15.82it/s]\n","epoch 334 iter 22: train loss 0.77819. lr 5.985359e-03: 100% 23/23 [00:01<00:00, 14.44it/s]\n","epoch 335 iter 22: train loss 0.76611. lr 3.268427e-03: 100% 23/23 [00:01<00:00, 14.65it/s]\n","epoch 336 iter 22: train loss 0.70664. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.45it/s]\n","epoch 337 iter 22: train loss 0.68535. lr 2.786833e-03: 100% 23/23 [00:01<00:00, 16.99it/s]\n","epoch 338 iter 22: train loss 0.81141. lr 5.994259e-03: 100% 23/23 [00:01<00:00, 16.94it/s]\n","epoch 339 iter 22: train loss 0.77591. lr 3.157835e-03: 100% 23/23 [00:01<00:00, 17.14it/s]\n","epoch 340 iter 22: train loss 0.70341. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 17.14it/s]\n","epoch 341 iter 22: train loss 0.68929. lr 2.897551e-03: 100% 23/23 [00:01<00:00, 16.69it/s]\n","epoch 342 iter 22: train loss 0.78974. lr 5.999069e-03: 100% 23/23 [00:01<00:00, 17.27it/s]\n","epoch 343 iter 22: train loss 0.78006. lr 3.047027e-03: 100% 23/23 [00:01<00:00, 14.77it/s]\n","epoch 344 iter 22: train loss 0.67953. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 13.99it/s]\n","epoch 345 iter 22: train loss 0.65718. lr 3.008410e-03: 100% 23/23 [00:01<00:00, 14.46it/s]\n","epoch 346 iter 22: train loss 0.81663. lr 5.999782e-03: 100% 23/23 [00:01<00:00, 14.87it/s]\n","epoch 347 iter 22: train loss 0.73678. lr 2.936156e-03: 100% 23/23 [00:01<00:00, 17.38it/s]\n","epoch 348 iter 22: train loss 0.69046. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 17.08it/s]\n","epoch 349 iter 22: train loss 0.68996. lr 3.119257e-03: 100% 23/23 [00:01<00:00, 16.55it/s]\n","epoch 350 iter 22: train loss 0.78636. lr 5.996399e-03: 100% 23/23 [00:01<00:00, 17.05it/s]\n","epoch 351 iter 22: train loss 0.73536. lr 2.825371e-03: 100% 23/23 [00:01<00:00, 17.05it/s]\n","epoch 352 iter 22: train loss 0.68536. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 16.80it/s]\n","epoch 353 iter 22: train loss 0.69772. lr 3.229941e-03: 100% 23/23 [00:01<00:00, 15.73it/s]\n","epoch 354 iter 22: train loss 0.76650. lr 5.988923e-03: 100% 23/23 [00:01<00:00, 14.40it/s]\n","epoch 355 iter 22: train loss 0.78674. lr 2.714825e-03: 100% 23/23 [00:01<00:00, 14.62it/s]\n","epoch 356 iter 22: train loss 0.69439. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 13.77it/s]\n","epoch 357 iter 22: train loss 0.70558. lr 3.340311e-03: 100% 23/23 [00:01<00:00, 16.97it/s]\n","epoch 358 iter 22: train loss 0.78841. lr 5.977364e-03: 100% 23/23 [00:01<00:00, 17.15it/s]\n","epoch 359 iter 22: train loss 0.75178. lr 2.604668e-03: 100% 23/23 [00:01<00:00, 17.37it/s]\n","epoch 360 iter 22: train loss 0.66780. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 16.70it/s]\n","epoch 361 iter 22: train loss 0.66112. lr 3.450217e-03: 100% 23/23 [00:01<00:00, 17.08it/s]\n","epoch 362 iter 22: train loss 0.76673. lr 5.961739e-03: 100% 23/23 [00:01<00:00, 16.73it/s]\n","epoch 363 iter 22: train loss 0.74046. lr 2.495052e-03: 100% 23/23 [00:01<00:00, 15.79it/s]\n","epoch 364 iter 22: train loss 0.67631. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.10it/s]\n","epoch 365 iter 22: train loss 0.67907. lr 3.559507e-03: 100% 23/23 [00:01<00:00, 14.47it/s]\n","epoch 366 iter 22: train loss 0.80045. lr 5.942068e-03: 100% 23/23 [00:01<00:00, 14.64it/s]\n","epoch 367 iter 22: train loss 0.74612. lr 2.386125e-03: 100% 23/23 [00:01<00:00, 16.99it/s]\n","epoch 368 iter 22: train loss 0.68663. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 17.17it/s]\n","epoch 369 iter 22: train loss 0.71547. lr 3.668033e-03: 100% 23/23 [00:01<00:00, 16.48it/s]\n","epoch 370 iter 22: train loss 0.77202. lr 5.918379e-03: 100% 23/23 [00:01<00:00, 17.02it/s]\n","epoch 371 iter 22: train loss 0.76002. lr 2.278036e-03: 100% 23/23 [00:01<00:00, 16.96it/s]\n","epoch 372 iter 22: train loss 0.67599. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 16.78it/s]\n","epoch 373 iter 22: train loss 0.72469. lr 3.775647e-03: 100% 23/23 [00:01<00:00, 15.70it/s]\n","epoch 374 iter 22: train loss 0.77123. lr 5.890704e-03: 100% 23/23 [00:01<00:00, 13.93it/s]\n","epoch 375 iter 22: train loss 0.76662. lr 2.170934e-03: 100% 23/23 [00:01<00:00, 14.32it/s]\n","epoch 376 iter 22: train loss 0.65680. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.85it/s]\n","epoch 377 iter 22: train loss 0.70224. lr 3.882202e-03: 100% 23/23 [00:01<00:00, 17.25it/s]\n","epoch 378 iter 22: train loss 0.76418. lr 5.859081e-03: 100% 23/23 [00:01<00:00, 17.09it/s]\n","epoch 379 iter 22: train loss 0.72874. lr 2.064964e-03: 100% 23/23 [00:01<00:00, 16.85it/s]\n","epoch 380 iter 22: train loss 0.66825. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 16.87it/s]\n","epoch 381 iter 22: train loss 0.67325. lr 3.987551e-03: 100% 23/23 [00:01<00:00, 17.03it/s]\n","epoch 382 iter 22: train loss 0.79913. lr 5.823552e-03: 100% 23/23 [00:01<00:00, 16.73it/s]\n","epoch 383 iter 22: train loss 0.71145. lr 1.960271e-03: 100% 23/23 [00:01<00:00, 16.35it/s]\n","epoch 384 iter 22: train loss 0.68229. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.23it/s]\n","epoch 385 iter 22: train loss 0.66079. lr 4.091552e-03: 100% 23/23 [00:01<00:00, 14.28it/s]\n","epoch 386 iter 22: train loss 0.76002. lr 5.784167e-03: 100% 23/23 [00:01<00:00, 14.05it/s]\n","epoch 387 iter 22: train loss 0.73828. lr 1.856998e-03: 100% 23/23 [00:01<00:00, 16.93it/s]\n","epoch 388 iter 22: train loss 0.68644. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 16.84it/s]\n","epoch 389 iter 22: train loss 0.69289. lr 4.194061e-03: 100% 23/23 [00:01<00:00, 16.99it/s]\n","epoch 390 iter 22: train loss 0.79527. lr 5.740979e-03: 100% 23/23 [00:01<00:00, 16.85it/s]\n","epoch 391 iter 22: train loss 0.73156. lr 1.755287e-03: 100% 23/23 [00:01<00:00, 16.96it/s]\n","epoch 392 iter 22: train loss 0.65228. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 16.95it/s]\n","epoch 393 iter 22: train loss 0.67866. lr 4.294940e-03: 100% 23/23 [00:01<00:00, 15.82it/s]\n","epoch 394 iter 22: train loss 0.78218. lr 5.694048e-03: 100% 23/23 [00:01<00:00, 14.18it/s]\n","epoch 395 iter 22: train loss 0.71800. lr 1.655275e-03: 100% 23/23 [00:01<00:00, 14.21it/s]\n","epoch 396 iter 22: train loss 0.64687. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.91it/s]\n","epoch 397 iter 22: train loss 0.64962. lr 4.394050e-03: 100% 23/23 [00:01<00:00, 16.66it/s]\n","epoch 398 iter 22: train loss 0.81006. lr 5.643437e-03: 100% 23/23 [00:01<00:00, 16.93it/s]\n","epoch 399 iter 22: train loss 0.71649. lr 1.557101e-03: 100% 23/23 [00:01<00:00, 17.16it/s]\n","epoch 400 iter 22: train loss 0.66589. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 17.06it/s]\n","epoch 401 iter 22: train loss 0.69215. lr 4.491256e-03: 100% 23/23 [00:01<00:00, 16.79it/s]\n","epoch 402 iter 22: train loss 0.81958. lr 5.589215e-03: 100% 23/23 [00:01<00:00, 16.71it/s]\n","epoch 403 iter 22: train loss 0.75092. lr 1.460897e-03: 100% 23/23 [00:01<00:00, 14.71it/s]\n","epoch 404 iter 22: train loss 0.63075. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.19it/s]\n","epoch 405 iter 22: train loss 0.69139. lr 4.586425e-03: 100% 23/23 [00:01<00:00, 14.32it/s]\n","epoch 406 iter 22: train loss 0.80371. lr 5.531457e-03: 100% 23/23 [00:01<00:00, 16.34it/s]\n","epoch 407 iter 22: train loss 0.69525. lr 1.366795e-03: 100% 23/23 [00:01<00:00, 16.71it/s]\n","epoch 408 iter 22: train loss 0.68345. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 17.00it/s]\n","epoch 409 iter 22: train loss 0.68269. lr 4.679428e-03: 100% 23/23 [00:01<00:00, 16.82it/s]\n","epoch 410 iter 22: train loss 0.77861. lr 5.470241e-03: 100% 23/23 [00:01<00:00, 16.55it/s]\n","epoch 411 iter 22: train loss 0.71684. lr 1.274924e-03: 100% 23/23 [00:01<00:00, 17.08it/s]\n","epoch 412 iter 22: train loss 0.64419. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 15.90it/s]\n","epoch 413 iter 22: train loss 0.66370. lr 4.770136e-03: 100% 23/23 [00:01<00:00, 14.28it/s]\n","epoch 414 iter 22: train loss 0.80274. lr 5.405651e-03: 100% 23/23 [00:01<00:00, 14.33it/s]\n","epoch 415 iter 22: train loss 0.71575. lr 1.185409e-03: 100% 23/23 [00:01<00:00, 15.95it/s]\n","epoch 416 iter 22: train loss 0.65655. lr 6.278815e-04: 100% 23/23 [00:01<00:00, 16.77it/s]\n","epoch 417 iter 22: train loss 0.68230. lr 4.858427e-03: 100% 23/23 [00:01<00:00, 17.09it/s]\n","epoch 418 iter 22: train loss 0.75763. lr 5.337776e-03: 100% 23/23 [00:01<00:00, 14.61it/s]\n","epoch 419 iter 22: train loss 0.71503. lr 1.098372e-03: 100% 23/23 [00:01<00:00, 16.78it/s]\n","epoch 420 iter 22: train loss 0.68593. lr 6.973654e-04: 100% 23/23 [00:01<00:00, 16.86it/s]\n","epoch 421 iter 22: train loss 0.71679. lr 4.944179e-03: 100% 23/23 [00:01<00:00, 16.25it/s]\n","epoch 422 iter 22: train loss 0.78808. lr 5.266707e-03: 100% 23/23 [00:01<00:00, 14.56it/s]\n","epoch 423 iter 22: train loss 0.68341. lr 1.013933e-03: 100% 23/23 [00:01<00:00, 14.49it/s]\n","epoch 424 iter 22: train loss 0.63994. lr 7.699944e-04: 100% 23/23 [00:01<00:00, 14.18it/s]\n","epoch 425 iter 22: train loss 0.69813. lr 5.027276e-03: 100% 23/23 [00:01<00:00, 16.88it/s]\n","epoch 426 iter 22: train loss 0.77487. lr 5.192543e-03: 100% 23/23 [00:01<00:00, 16.94it/s]\n","epoch 427 iter 22: train loss 0.70306. lr 9.322066e-04: 100% 23/23 [00:01<00:00, 17.15it/s]\n","epoch 428 iter 22: train loss 0.63334. lr 8.456692e-04: 100% 23/23 [00:01<00:00, 17.22it/s]\n","epoch 429 iter 22: train loss 0.66863. lr 5.107604e-03: 100% 23/23 [00:01<00:00, 16.94it/s]\n","epoch 430 iter 22: train loss 0.77125. lr 5.115383e-03: 100% 23/23 [00:01<00:00, 16.24it/s]\n","epoch 431 iter 22: train loss 0.68807. lr 8.533044e-04: 100% 23/23 [00:01<00:00, 14.54it/s]\n","epoch 432 iter 22: train loss 0.65616. lr 9.242865e-04: 100% 23/23 [00:01<00:00, 14.01it/s]\n","epoch 433 iter 22: train loss 0.67577. lr 5.185054e-03: 100% 23/23 [00:01<00:00, 12.65it/s]\n","epoch 434 iter 22: train loss 0.77206. lr 5.035335e-03: 100% 23/23 [00:01<00:00, 16.64it/s]\n","epoch 435 iter 22: train loss 0.67766. lr 7.773343e-04: 100% 23/23 [00:01<00:00, 17.07it/s]\n","epoch 436 iter 22: train loss 0.62943. lr 1.005739e-03: 100% 23/23 [00:01<00:00, 16.70it/s]\n","epoch 437 iter 22: train loss 0.67881. lr 5.259519e-03: 100% 23/23 [00:01<00:00, 16.55it/s]\n","epoch 438 iter 22: train loss 0.77167. lr 4.952506e-03: 100% 23/23 [00:01<00:00, 16.74it/s]\n","epoch 439 iter 22: train loss 0.65811. lr 7.044001e-04: 100% 23/23 [00:01<00:00, 17.18it/s]\n","epoch 440 iter 22: train loss 0.62884. lr 1.089915e-03: 100% 23/23 [00:01<00:00, 15.67it/s]\n","epoch 441 iter 22: train loss 0.68615. lr 5.330897e-03: 100% 23/23 [00:01<00:00, 14.59it/s]\n","epoch 442 iter 22: train loss 0.73169. lr 4.867011e-03: 100% 23/23 [00:01<00:00, 14.13it/s]\n","epoch 443 iter 22: train loss 0.67594. lr 6.346012e-04: 100% 23/23 [00:01<00:00, 15.47it/s]\n","epoch 444 iter 22: train loss 0.61943. lr 1.176700e-03: 100% 23/23 [00:01<00:00, 17.08it/s]\n","epoch 445 iter 22: train loss 0.69754. lr 5.399092e-03: 100% 23/23 [00:01<00:00, 16.69it/s]\n","epoch 446 iter 22: train loss 0.76089. lr 4.778966e-03: 100% 23/23 [00:01<00:00, 16.73it/s]\n","epoch 447 iter 22: train loss 0.67067. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 16.98it/s]\n","epoch 448 iter 22: train loss 0.60529. lr 1.265976e-03: 100% 23/23 [00:01<00:00, 16.59it/s]\n","epoch 449 iter 22: train loss 0.67522. lr 5.464011e-03: 100% 23/23 [00:01<00:00, 16.92it/s]\n","epoch 450 iter 22: train loss 0.77957. lr 4.688490e-03: 100% 23/23 [00:01<00:00, 14.09it/s]\n","epoch 451 iter 22: train loss 0.67601. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.44it/s]\n","epoch 452 iter 22: train loss 0.59614. lr 1.357620e-03: 100% 23/23 [00:01<00:00, 14.69it/s]\n","epoch 453 iter 22: train loss 0.70228. lr 5.525563e-03: 100% 23/23 [00:01<00:00, 17.21it/s]\n","epoch 454 iter 22: train loss 0.74176. lr 4.595709e-03: 100% 23/23 [00:01<00:00, 17.03it/s]\n","epoch 455 iter 22: train loss 0.64593. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 17.00it/s]\n","epoch 456 iter 22: train loss 0.62405. lr 1.451507e-03: 100% 23/23 [00:01<00:00, 14.67it/s]\n","epoch 457 iter 22: train loss 0.70682. lr 5.583667e-03: 100% 23/23 [00:01<00:00, 16.82it/s]\n","epoch 458 iter 22: train loss 0.73647. lr 4.500748e-03: 100% 23/23 [00:01<00:00, 17.05it/s]\n","epoch 459 iter 22: train loss 0.66757. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 15.61it/s]\n","epoch 460 iter 22: train loss 0.62878. lr 1.547509e-03: 100% 23/23 [00:01<00:00, 14.60it/s]\n","epoch 461 iter 22: train loss 0.72094. lr 5.638241e-03: 100% 23/23 [00:01<00:00, 14.14it/s]\n","epoch 462 iter 22: train loss 0.75961. lr 4.403737e-03: 100% 23/23 [00:01<00:00, 16.09it/s]\n","epoch 463 iter 22: train loss 0.65448. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 16.94it/s]\n","epoch 464 iter 22: train loss 0.62570. lr 1.645495e-03: 100% 23/23 [00:01<00:00, 16.79it/s]\n","epoch 465 iter 22: train loss 0.71417. lr 5.689212e-03: 100% 23/23 [00:01<00:00, 17.04it/s]\n","epoch 466 iter 22: train loss 0.75955. lr 4.304809e-03: 100% 23/23 [00:01<00:00, 16.63it/s]\n","epoch 467 iter 22: train loss 0.69284. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 16.41it/s]\n","epoch 468 iter 22: train loss 0.60863. lr 1.745332e-03: 100% 23/23 [00:01<00:00, 16.35it/s]\n","epoch 469 iter 22: train loss 0.68970. lr 5.736510e-03: 100% 23/23 [00:01<00:00, 14.38it/s]\n","epoch 470 iter 22: train loss 0.73951. lr 4.204099e-03: 100% 23/23 [00:01<00:00, 14.43it/s]\n","epoch 471 iter 22: train loss 0.62586. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.58it/s]\n","epoch 472 iter 22: train loss 0.65159. lr 1.846881e-03: 100% 23/23 [00:01<00:00, 17.11it/s]\n","epoch 473 iter 22: train loss 0.68994. lr 5.780070e-03: 100% 23/23 [00:01<00:00, 16.86it/s]\n","epoch 474 iter 22: train loss 0.73929. lr 4.101744e-03: 100% 23/23 [00:01<00:00, 17.03it/s]\n","epoch 475 iter 22: train loss 0.64926. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 15.67it/s]\n","epoch 476 iter 22: train loss 0.58102. lr 1.950006e-03: 100% 23/23 [00:01<00:00, 16.35it/s]\n","epoch 477 iter 22: train loss 0.75292. lr 5.819833e-03: 100% 23/23 [00:01<00:00, 17.32it/s]\n","epoch 478 iter 22: train loss 0.72467. lr 3.997885e-03: 100% 23/23 [00:01<00:00, 13.95it/s]\n","epoch 479 iter 22: train loss 0.65355. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.43it/s]\n","epoch 480 iter 22: train loss 0.62167. lr 2.054565e-03: 100% 23/23 [00:01<00:00, 14.31it/s]\n","epoch 481 iter 22: train loss 0.69596. lr 5.855745e-03: 100% 23/23 [00:01<00:00, 17.30it/s]\n","epoch 482 iter 22: train loss 0.74660. lr 3.892662e-03: 100% 23/23 [00:01<00:00, 16.66it/s]\n","epoch 483 iter 22: train loss 0.69618. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 17.24it/s]\n","epoch 484 iter 22: train loss 0.64622. lr 2.160415e-03: 100% 23/23 [00:01<00:00, 16.70it/s]\n","epoch 485 iter 22: train loss 0.72617. lr 5.887756e-03: 100% 23/23 [00:01<00:00, 16.34it/s]\n","epoch 486 iter 22: train loss 0.76438. lr 3.786220e-03: 100% 23/23 [00:01<00:00, 17.04it/s]\n","epoch 487 iter 22: train loss 0.64524. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 15.70it/s]\n","epoch 488 iter 22: train loss 0.60097. lr 2.267412e-03: 100% 23/23 [00:01<00:00, 14.23it/s]\n","epoch 489 iter 22: train loss 0.69417. lr 5.915823e-03: 100% 23/23 [00:01<00:00, 14.30it/s]\n","epoch 490 iter 22: train loss 0.70003. lr 3.678705e-03: 100% 23/23 [00:01<00:00, 16.01it/s]\n","epoch 491 iter 22: train loss 0.66151. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 16.75it/s]\n","epoch 492 iter 22: train loss 0.59421. lr 2.375410e-03: 100% 23/23 [00:01<00:00, 16.92it/s]\n","epoch 493 iter 22: train loss 0.73546. lr 5.939907e-03: 100% 23/23 [00:01<00:00, 16.22it/s]\n","epoch 494 iter 22: train loss 0.73204. lr 3.570262e-03: 100% 23/23 [00:01<00:00, 16.39it/s]\n","epoch 495 iter 22: train loss 0.64506. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.76it/s]\n","epoch 496 iter 22: train loss 0.60314. lr 2.484261e-03: 100% 23/23 [00:01<00:00, 16.64it/s]\n","epoch 497 iter 22: train loss 0.67813. lr 5.959976e-03: 100% 23/23 [00:01<00:00, 14.06it/s]\n","epoch 498 iter 22: train loss 0.71885. lr 3.461040e-03: 100% 23/23 [00:01<00:00, 14.25it/s]\n","epoch 499 iter 22: train loss 0.62076. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 15.24it/s]\n","epoch 500 iter 22: train loss 0.59297. lr 2.593816e-03: 100% 23/23 [00:01<00:00, 17.04it/s]\n","epoch 501 iter 22: train loss 0.71457. lr 5.976001e-03: 100% 23/23 [00:01<00:00, 16.48it/s]\n","epoch 502 iter 22: train loss 0.69544. lr 3.351189e-03: 100% 23/23 [00:01<00:00, 16.82it/s]\n","epoch 503 iter 22: train loss 0.66311. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 16.61it/s]\n","epoch 504 iter 22: train loss 0.61659. lr 2.703926e-03: 100% 23/23 [00:01<00:00, 16.72it/s]\n","epoch 505 iter 22: train loss 0.70095. lr 5.987962e-03: 100% 23/23 [00:01<00:00, 17.04it/s]\n","epoch 506 iter 22: train loss 0.71224. lr 3.240858e-03: 100% 23/23 [00:01<00:00, 14.51it/s]\n","epoch 507 iter 22: train loss 0.64060. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.45it/s]\n","epoch 508 iter 22: train loss 0.62397. lr 2.814440e-03: 100% 23/23 [00:01<00:00, 14.41it/s]\n","epoch 509 iter 22: train loss 0.73323. lr 5.995842e-03: 100% 23/23 [00:01<00:00, 16.76it/s]\n","epoch 510 iter 22: train loss 0.71528. lr 3.130198e-03: 100% 23/23 [00:01<00:00, 16.91it/s]\n","epoch 511 iter 22: train loss 0.61561. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 16.81it/s]\n","epoch 512 iter 22: train loss 0.61678. lr 2.925208e-03: 100% 23/23 [00:01<00:00, 16.73it/s]\n","epoch 513 iter 22: train loss 0.72859. lr 5.999631e-03: 100% 23/23 [00:01<00:00, 16.54it/s]\n","epoch 514 iter 22: train loss 0.72426. lr 3.019360e-03: 100% 23/23 [00:01<00:00, 15.16it/s]\n","epoch 515 iter 22: train loss 0.61097. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 15.76it/s]\n","epoch 516 iter 22: train loss 0.62706. lr 3.036078e-03: 100% 23/23 [00:01<00:00, 14.08it/s]\n","epoch 517 iter 22: train loss 0.72280. lr 5.999322e-03: 100% 23/23 [00:01<00:00, 13.84it/s]\n","epoch 518 iter 22: train loss 0.69779. lr 2.908496e-03: 100% 23/23 [00:01<00:00, 15.89it/s]\n","epoch 519 iter 22: train loss 0.62078. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 16.93it/s]\n","epoch 520 iter 22: train loss 0.62507. lr 3.146898e-03: 100% 23/23 [00:01<00:00, 17.01it/s]\n","epoch 521 iter 22: train loss 0.73182. lr 5.994916e-03: 100% 23/23 [00:01<00:00, 16.78it/s]\n","epoch 522 iter 22: train loss 0.69876. lr 2.797757e-03: 100% 23/23 [00:01<00:00, 16.90it/s]\n","epoch 523 iter 22: train loss 0.63379. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 16.68it/s]\n","epoch 524 iter 22: train loss 0.58604. lr 3.257519e-03: 100% 23/23 [00:01<00:00, 16.09it/s]\n","epoch 525 iter 22: train loss 0.67503. lr 5.986420e-03: 100% 23/23 [00:01<00:00, 14.54it/s]\n","epoch 526 iter 22: train loss 0.71417. lr 2.687294e-03: 100% 23/23 [00:01<00:00, 14.20it/s]\n","epoch 527 iter 22: train loss 0.64066. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 15.11it/s]\n","epoch 528 iter 22: train loss 0.64148. lr 3.367787e-03: 100% 23/23 [00:01<00:00, 17.07it/s]\n","epoch 529 iter 22: train loss 0.77318. lr 5.973845e-03: 100% 23/23 [00:01<00:00, 16.85it/s]\n","epoch 530 iter 22: train loss 0.73476. lr 2.577258e-03: 100% 23/23 [00:01<00:00, 16.88it/s]\n","epoch 531 iter 22: train loss 0.63576. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 17.21it/s]\n","epoch 532 iter 22: train loss 0.62021. lr 3.477553e-03: 100% 23/23 [00:01<00:00, 16.88it/s]\n","epoch 533 iter 22: train loss 0.72321. lr 5.957208e-03: 100% 23/23 [00:01<00:00, 16.17it/s]\n","epoch 534 iter 22: train loss 0.67353. lr 2.467800e-03: 100% 23/23 [00:01<00:00, 12.44it/s]\n","epoch 535 iter 22: train loss 0.62687. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.54it/s]\n","epoch 536 iter 22: train loss 0.61253. lr 3.586666e-03: 100% 23/23 [00:01<00:00, 14.40it/s]\n","epoch 537 iter 22: train loss 0.71857. lr 5.936532e-03: 100% 23/23 [00:01<00:00, 16.90it/s]\n","epoch 538 iter 22: train loss 0.68833. lr 2.359068e-03: 100% 23/23 [00:01<00:00, 16.87it/s]\n","epoch 539 iter 22: train loss 0.62183. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 16.75it/s]\n","epoch 540 iter 22: train loss 0.61462. lr 3.694979e-03: 100% 23/23 [00:01<00:00, 16.47it/s]\n","epoch 541 iter 22: train loss 0.70428. lr 5.911845e-03: 100% 23/23 [00:01<00:00, 16.75it/s]\n","epoch 542 iter 22: train loss 0.71948. lr 2.251212e-03: 100% 23/23 [00:01<00:00, 16.98it/s]\n","epoch 543 iter 22: train loss 0.64155. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 16.10it/s]\n","epoch 544 iter 22: train loss 0.58978. lr 3.802342e-03: 100% 23/23 [00:01<00:00, 14.39it/s]\n","epoch 545 iter 22: train loss 0.74300. lr 5.883181e-03: 100% 23/23 [00:01<00:00, 14.22it/s]\n","epoch 546 iter 22: train loss 0.66560. lr 2.144378e-03: 100% 23/23 [00:01<00:00, 15.50it/s]\n","epoch 547 iter 22: train loss 0.62935. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 16.93it/s]\n","epoch 548 iter 22: train loss 0.60433. lr 3.908609e-03: 100% 23/23 [00:01<00:00, 16.91it/s]\n","epoch 549 iter 22: train loss 0.71663. lr 5.850579e-03: 100% 23/23 [00:01<00:00, 16.61it/s]\n","epoch 550 iter 22: train loss 0.66576. lr 2.038714e-03: 100% 23/23 [00:01<00:00, 17.02it/s]\n","epoch 551 iter 22: train loss 0.63721. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 16.87it/s]\n","epoch 552 iter 22: train loss 0.62748. lr 4.013635e-03: 100% 23/23 [00:01<00:00, 16.12it/s]\n","epoch 553 iter 22: train loss 0.71564. lr 5.814083e-03: 100% 23/23 [00:01<00:00, 14.24it/s]\n","epoch 554 iter 22: train loss 0.70364. lr 1.934362e-03: 100% 23/23 [00:01<00:00, 14.31it/s]\n","epoch 555 iter 22: train loss 0.60905. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.23it/s]\n","epoch 556 iter 22: train loss 0.62453. lr 4.117277e-03: 100% 23/23 [00:01<00:00, 16.92it/s]\n","epoch 557 iter 22: train loss 0.74118. lr 5.773744e-03: 100% 23/23 [00:01<00:00, 17.09it/s]\n","epoch 558 iter 22: train loss 0.66984. lr 1.831465e-03: 100% 23/23 [00:01<00:00, 16.41it/s]\n","epoch 559 iter 22: train loss 0.61971. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 16.74it/s]\n","epoch 560 iter 22: train loss 0.61479. lr 4.219393e-03: 100% 23/23 [00:01<00:00, 16.88it/s]\n","epoch 561 iter 22: train loss 0.71133. lr 5.729616e-03: 100% 23/23 [00:01<00:00, 16.71it/s]\n","epoch 562 iter 22: train loss 0.66423. lr 1.730165e-03: 100% 23/23 [00:01<00:00, 15.67it/s]\n","epoch 563 iter 22: train loss 0.61700. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 13.16it/s]\n","epoch 564 iter 22: train loss 0.64547. lr 4.319843e-03: 100% 23/23 [00:01<00:00, 14.42it/s]\n","epoch 565 iter 22: train loss 0.72563. lr 5.681760e-03: 100% 23/23 [00:01<00:00, 15.91it/s]\n","epoch 566 iter 22: train loss 0.70704. lr 1.630599e-03: 100% 23/23 [00:01<00:00, 16.61it/s]\n","epoch 567 iter 22: train loss 0.63167. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 17.15it/s]\n","epoch 568 iter 22: train loss 0.61188. lr 4.418491e-03: 100% 23/23 [00:01<00:00, 17.20it/s]\n","epoch 569 iter 22: train loss 0.70343. lr 5.630241e-03: 100% 23/23 [00:01<00:00, 17.09it/s]\n","epoch 570 iter 22: train loss 0.63238. lr 1.532904e-03: 100% 23/23 [00:01<00:00, 17.17it/s]\n","epoch 571 iter 22: train loss 0.58816. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 16.18it/s]\n","epoch 572 iter 22: train loss 0.60065. lr 4.515201e-03: 100% 23/23 [00:02<00:00, 10.95it/s]\n","epoch 573 iter 22: train loss 0.70259. lr 5.575130e-03: 100% 23/23 [00:01<00:00, 14.13it/s]\n","epoch 574 iter 22: train loss 0.67644. lr 1.437212e-03: 100% 23/23 [00:01<00:00, 15.39it/s]\n","epoch 575 iter 22: train loss 0.60974. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 16.69it/s]\n","epoch 576 iter 22: train loss 0.62025. lr 4.609841e-03: 100% 23/23 [00:01<00:00, 16.83it/s]\n","epoch 577 iter 22: train loss 0.74817. lr 5.516501e-03: 100% 23/23 [00:01<00:00, 16.71it/s]\n","epoch 578 iter 22: train loss 0.61323. lr 1.343655e-03: 100% 23/23 [00:01<00:00, 16.47it/s]\n","epoch 579 iter 22: train loss 0.62057. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 17.24it/s]\n","epoch 580 iter 22: train loss 0.62157. lr 4.702283e-03: 100% 23/23 [00:01<00:00, 16.60it/s]\n","epoch 581 iter 22: train loss 0.75963. lr 5.454436e-03: 100% 23/23 [00:01<00:00, 14.84it/s]\n","epoch 582 iter 22: train loss 0.64223. lr 1.252360e-03: 100% 23/23 [00:01<00:00, 14.54it/s]\n","epoch 583 iter 22: train loss 0.64104. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.06it/s]\n","epoch 584 iter 22: train loss 0.61708. lr 4.792400e-03: 100% 23/23 [00:01<00:00, 16.98it/s]\n","epoch 585 iter 22: train loss 0.71116. lr 5.389018e-03: 100% 23/23 [00:01<00:00, 16.85it/s]\n","epoch 586 iter 22: train loss 0.64631. lr 1.163453e-03: 100% 23/23 [00:01<00:00, 16.66it/s]\n","epoch 587 iter 22: train loss 0.60685. lr 6.449209e-04: 100% 23/23 [00:01<00:00, 16.82it/s]\n","epoch 588 iter 22: train loss 0.64107. lr 4.880068e-03: 100% 23/23 [00:01<00:00, 17.14it/s]\n","epoch 589 iter 22: train loss 0.74297. lr 5.320336e-03: 100% 23/23 [00:01<00:00, 16.55it/s]\n","epoch 590 iter 22: train loss 0.63035. lr 1.077053e-03: 100% 23/23 [00:01<00:00, 15.56it/s]\n","epoch 591 iter 22: train loss 0.63583. lr 7.151987e-04: 100% 23/23 [00:01<00:00, 11.63it/s]\n","epoch 592 iter 22: train loss 0.60828. lr 4.965169e-03: 100% 23/23 [00:01<00:00, 13.37it/s]\n","epoch 593 iter 22: train loss 0.73315. lr 5.248486e-03: 100% 23/23 [00:01<00:00, 15.38it/s]\n","epoch 594 iter 22: train loss 0.71381. lr 9.932805e-04: 100% 23/23 [00:01<00:00, 15.38it/s]\n","epoch 595 iter 22: train loss 0.61865. lr 7.885972e-04: 100% 23/23 [00:01<00:00, 14.33it/s]\n","epoch 596 iter 22: train loss 0.64240. lr 5.047585e-03: 100% 23/23 [00:01<00:00, 13.89it/s]\n","epoch 597 iter 22: train loss 0.70477. lr 5.173564e-03: 100% 23/23 [00:01<00:00, 13.95it/s]\n","epoch 598 iter 22: train loss 0.61801. lr 9.122485e-04: 100% 23/23 [00:01<00:00, 16.80it/s]\n","epoch 599 iter 22: train loss 0.60416. lr 8.650161e-04: 100% 23/23 [00:01<00:00, 14.02it/s]\n","epoch 600 iter 22: train loss 0.64136. lr 5.127205e-03: 100% 23/23 [00:01<00:00, 14.08it/s]\n","epoch 601 iter 22: train loss 0.71769. lr 5.095674e-03: 100% 23/23 [00:01<00:00, 14.60it/s]\n","epoch 602 iter 22: train loss 0.64333. lr 8.340680e-04: 100% 23/23 [00:01<00:00, 16.85it/s]\n","epoch 603 iter 22: train loss 0.59287. lr 9.443511e-04: 100% 23/23 [00:01<00:00, 16.82it/s]\n","epoch 604 iter 22: train loss 0.61315. lr 5.203919e-03: 100% 23/23 [00:01<00:00, 16.89it/s]\n","epoch 605 iter 22: train loss 0.69966. lr 5.014922e-03: 100% 23/23 [00:01<00:00, 17.03it/s]\n","epoch 606 iter 22: train loss 0.65866. lr 7.588460e-04: 100% 23/23 [00:01<00:00, 17.07it/s]\n","epoch 607 iter 22: train loss 0.60635. lr 1.026494e-03: 100% 23/23 [00:01<00:00, 16.70it/s]\n","epoch 608 iter 22: train loss 0.65016. lr 5.277623e-03: 100% 23/23 [00:01<00:00, 15.58it/s]\n","epoch 609 iter 22: train loss 0.75099. lr 4.931417e-03: 100% 23/23 [00:01<00:00, 12.75it/s]\n","epoch 610 iter 22: train loss 0.63965. lr 6.866850e-04: 100% 23/23 [00:01<00:00, 14.26it/s]\n","epoch 611 iter 22: train loss 0.56590. lr 1.111332e-03: 100% 23/23 [00:01<00:00, 15.72it/s]\n","epoch 612 iter 22: train loss 0.66732. lr 5.348217e-03: 100% 23/23 [00:01<00:00, 16.58it/s]\n","epoch 613 iter 22: train loss 0.74408. lr 4.845274e-03: 100% 23/23 [00:01<00:00, 16.84it/s]\n","epoch 614 iter 22: train loss 0.64050. lr 6.176836e-04: 100% 23/23 [00:01<00:00, 16.90it/s]\n","epoch 615 iter 22: train loss 0.57708. lr 1.198750e-03: 100% 23/23 [00:01<00:00, 17.23it/s]\n","epoch 616 iter 22: train loss 0.63221. lr 5.415603e-03: 100% 23/23 [00:01<00:00, 16.64it/s]\n","epoch 617 iter 22: train loss 0.69539. lr 4.756611e-03: 100% 23/23 [00:01<00:00, 16.28it/s]\n","epoch 618 iter 22: train loss 0.64942. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.13it/s]\n","epoch 619 iter 22: train loss 0.61529. lr 1.288628e-03: 100% 23/23 [00:01<00:00, 14.23it/s]\n","epoch 620 iter 22: train loss 0.60974. lr 5.479689e-03: 100% 23/23 [00:01<00:00, 14.69it/s]\n","epoch 621 iter 22: train loss 0.71538. lr 4.665549e-03: 100% 23/23 [00:01<00:00, 17.03it/s]\n","epoch 622 iter 22: train loss 0.60762. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 16.71it/s]\n","epoch 623 iter 22: train loss 0.59533. lr 1.380844e-03: 100% 23/23 [00:01<00:00, 17.23it/s]\n","epoch 624 iter 22: train loss 0.65218. lr 5.540389e-03: 100% 23/23 [00:01<00:00, 16.54it/s]\n","epoch 625 iter 22: train loss 0.73613. lr 4.572211e-03: 100% 23/23 [00:01<00:00, 16.75it/s]\n","epoch 626 iter 22: train loss 0.59708. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 16.53it/s]\n","epoch 627 iter 22: train loss 0.58101. lr 1.475271e-03: 100% 23/23 [00:01<00:00, 14.99it/s]\n","epoch 628 iter 22: train loss 0.63648. lr 5.597619e-03: 100% 23/23 [00:02<00:00, 11.26it/s]\n","epoch 629 iter 22: train loss 0.68829. lr 4.476727e-03: 100% 23/23 [00:01<00:00, 14.11it/s]\n","epoch 630 iter 22: train loss 0.63101. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 16.99it/s]\n","epoch 631 iter 22: train loss 0.57485. lr 1.571780e-03: 100% 23/23 [00:01<00:00, 16.68it/s]\n","epoch 632 iter 22: train loss 0.65871. lr 5.651301e-03: 100% 23/23 [00:01<00:00, 17.26it/s]\n","epoch 633 iter 22: train loss 0.72458. lr 4.379225e-03: 100% 23/23 [00:01<00:00, 17.29it/s]\n","epoch 634 iter 22: train loss 0.60944. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 16.86it/s]\n","epoch 635 iter 22: train loss 0.59637. lr 1.670241e-03: 100% 23/23 [00:01<00:00, 17.19it/s]\n","epoch 636 iter 22: train loss 0.65183. lr 5.701361e-03: 100% 23/23 [00:01<00:00, 15.86it/s]\n","epoch 637 iter 22: train loss 0.71334. lr 4.279840e-03: 100% 23/23 [00:01<00:00, 14.82it/s]\n","epoch 638 iter 22: train loss 0.62998. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.16it/s]\n","epoch 639 iter 22: train loss 0.55524. lr 1.770517e-03: 100% 23/23 [00:01<00:00, 15.40it/s]\n","epoch 640 iter 22: train loss 0.65980. lr 5.747732e-03: 100% 23/23 [00:01<00:00, 16.78it/s]\n","epoch 641 iter 22: train loss 0.72686. lr 4.178706e-03: 100% 23/23 [00:01<00:00, 17.04it/s]\n","epoch 642 iter 22: train loss 0.62274. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 16.63it/s]\n","epoch 643 iter 22: train loss 0.59107. lr 1.872473e-03: 100% 23/23 [00:01<00:00, 16.10it/s]\n","epoch 644 iter 22: train loss 0.68439. lr 5.790350e-03: 100% 23/23 [00:01<00:00, 16.34it/s]\n","epoch 645 iter 22: train loss 0.67979. lr 4.075962e-03: 100% 23/23 [00:01<00:00, 16.61it/s]\n","epoch 646 iter 22: train loss 0.62173. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.12it/s]\n","epoch 647 iter 22: train loss 0.59228. lr 1.975969e-03: 100% 23/23 [00:01<00:00, 13.93it/s]\n","epoch 648 iter 22: train loss 0.66330. lr 5.829157e-03: 100% 23/23 [00:02<00:00, 11.50it/s]\n","epoch 649 iter 22: train loss 0.74355. lr 3.971749e-03: 100% 23/23 [00:01<00:00, 17.20it/s]\n","epoch 650 iter 22: train loss 0.62909. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 16.67it/s]\n","2023-12-19 17:03:36.552775: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2023-12-19 17:03:36.552825: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2023-12-19 17:03:36.554144: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2023-12-19 17:03:36.561487: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-12-19 17:03:37.644291: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","data has 418352 characters, 256 unique.\n","number of parameters: 3323392\n","epoch 1 iter 7: train loss 0.75605. lr 5.999844e-04: 100% 8/8 [00:02<00:00,  3.21it/s]\n","epoch 2 iter 7: train loss 0.55987. lr 5.999351e-04: 100% 8/8 [00:01<00:00,  7.96it/s]\n","epoch 3 iter 7: train loss 0.45410. lr 5.998521e-04: 100% 8/8 [00:00<00:00,  8.64it/s]\n","epoch 4 iter 7: train loss 0.37965. lr 5.997352e-04: 100% 8/8 [00:00<00:00,  9.24it/s]\n","epoch 5 iter 7: train loss 0.31036. lr 5.995847e-04: 100% 8/8 [00:00<00:00,  9.44it/s]\n","epoch 6 iter 7: train loss 0.26512. lr 5.994004e-04: 100% 8/8 [00:00<00:00,  9.43it/s]\n","epoch 7 iter 7: train loss 0.22528. lr 5.991823e-04: 100% 8/8 [00:00<00:00,  9.34it/s]\n","epoch 8 iter 7: train loss 0.18992. lr 5.989306e-04: 100% 8/8 [00:00<00:00,  9.13it/s]\n","epoch 9 iter 7: train loss 0.16096. lr 5.986453e-04: 100% 8/8 [00:00<00:00,  9.33it/s]\n","epoch 10 iter 7: train loss 0.12986. lr 5.983263e-04: 100% 8/8 [00:00<00:00,  8.99it/s]\n","2023-12-19 17:03:56.178957: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2023-12-19 17:03:56.179017: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2023-12-19 17:03:56.180964: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2023-12-19 17:03:56.191926: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-12-19 17:03:57.779835: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","data has 418352 characters, 256 unique.\n","number of parameters: 3323392\n","500it [00:52,  9.49it/s]\n","Correct: 148.0 out of 500.0: 29.599999999999998%\n","2023-12-19 17:04:57.802436: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2023-12-19 17:04:57.802505: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2023-12-19 17:04:57.804448: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2023-12-19 17:04:57.815547: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-12-19 17:04:59.387414: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","data has 418352 characters, 256 unique.\n","number of parameters: 3323392\n","437it [00:47,  9.11it/s]\n","No gold birth places provided; returning (0,0)\n","Predictions written to vanilla.pretrain.test.predictions; no targets provided\n"]}]},{"cell_type":"markdown","source":["Q 2g) Research! Write and try out a more efficient variant of Attention\n"],"metadata":{"id":"gGPO479PxFmE"}},{"cell_type":"code","source":["! bash scripts/run_perceiver.sh"],"metadata":{"id":"fnAS6t7GHxIV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1703007082869,"user_tz":-60,"elapsed":956156,"user":{"displayName":"Fida Fida","userId":"16516579540060451036"}},"outputId":"f362aff4-98e0-4d4d-be3b-71dc797dc688"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-12-19 17:15:28.333408: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2023-12-19 17:15:28.333464: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2023-12-19 17:15:28.334862: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2023-12-19 17:15:28.342584: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-12-19 17:15:29.450003: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","data has 418352 characters, 256 unique.\n","number of parameters: 3339776\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","epoch 1 iter 22: train loss 3.44604. lr 3.102347e-03: 100% 23/23 [00:02<00:00,  9.67it/s]\n","epoch 2 iter 22: train loss 3.40097. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 21.97it/s]\n","epoch 3 iter 22: train loss 3.43730. lr 2.953074e-03: 100% 23/23 [00:00<00:00, 25.17it/s]\n","epoch 4 iter 22: train loss 3.19347. lr 5.999939e-03: 100% 23/23 [00:00<00:00, 26.20it/s]\n","epoch 5 iter 22: train loss 3.02281. lr 2.991488e-03: 100% 23/23 [00:00<00:00, 25.94it/s]\n","epoch 6 iter 22: train loss 2.90487. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 25.94it/s]\n","epoch 7 iter 22: train loss 2.89504. lr 3.063946e-03: 100% 23/23 [00:00<00:00, 25.08it/s]\n","epoch 8 iter 22: train loss 2.86830. lr 5.998600e-03: 100% 23/23 [00:00<00:00, 25.71it/s]\n","epoch 9 iter 22: train loss 2.81017. lr 2.880641e-03: 100% 23/23 [00:00<00:00, 25.52it/s]\n","epoch 10 iter 22: train loss 2.79575. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 25.38it/s]\n","epoch 11 iter 22: train loss 2.78569. lr 3.174730e-03: 100% 23/23 [00:00<00:00, 23.25it/s]\n","epoch 12 iter 22: train loss 2.78308. lr 5.993165e-03: 100% 23/23 [00:01<00:00, 18.80it/s]\n","epoch 13 iter 22: train loss 2.71567. lr 2.769957e-03: 100% 23/23 [00:01<00:00, 19.49it/s]\n","epoch 14 iter 22: train loss 2.67934. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 19.15it/s]\n","epoch 15 iter 22: train loss 2.69793. lr 3.285276e-03: 100% 23/23 [00:01<00:00, 18.33it/s]\n","epoch 16 iter 22: train loss 2.69415. lr 5.983642e-03: 100% 23/23 [00:00<00:00, 25.23it/s]\n","epoch 17 iter 22: train loss 2.66607. lr 2.659588e-03: 100% 23/23 [00:00<00:00, 25.36it/s]\n","epoch 18 iter 22: train loss 2.62368. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 25.70it/s]\n","epoch 19 iter 22: train loss 2.63603. lr 3.395432e-03: 100% 23/23 [00:00<00:00, 25.05it/s]\n","epoch 20 iter 22: train loss 2.64267. lr 5.970044e-03: 100% 23/23 [00:00<00:00, 24.24it/s]\n","epoch 21 iter 22: train loss 2.63566. lr 2.549683e-03: 100% 23/23 [00:00<00:00, 24.97it/s]\n","epoch 22 iter 22: train loss 2.61280. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 25.09it/s]\n","epoch 23 iter 22: train loss 2.58870. lr 3.505048e-03: 100% 23/23 [00:00<00:00, 24.93it/s]\n","epoch 24 iter 22: train loss 2.62662. lr 5.952389e-03: 100% 23/23 [00:01<00:00, 21.35it/s]\n","epoch 25 iter 22: train loss 2.60666. lr 2.440393e-03: 100% 23/23 [00:01<00:00, 18.89it/s]\n","epoch 26 iter 22: train loss 2.59151. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 19.02it/s]\n","epoch 27 iter 22: train loss 2.58920. lr 3.613975e-03: 100% 23/23 [00:01<00:00, 18.53it/s]\n","epoch 28 iter 22: train loss 2.60602. lr 5.930702e-03: 100% 23/23 [00:01<00:00, 18.40it/s]\n","epoch 29 iter 22: train loss 2.58305. lr 2.331867e-03: 100% 23/23 [00:01<00:00, 20.61it/s]\n","epoch 30 iter 22: train loss 2.56479. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 25.22it/s]\n","epoch 31 iter 22: train loss 2.57093. lr 3.722062e-03: 100% 23/23 [00:00<00:00, 24.77it/s]\n","epoch 32 iter 22: train loss 2.58417. lr 5.905012e-03: 100% 23/23 [00:00<00:00, 25.31it/s]\n","epoch 33 iter 22: train loss 2.56107. lr 2.224255e-03: 100% 23/23 [00:00<00:00, 24.83it/s]\n","epoch 34 iter 22: train loss 2.53880. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 25.23it/s]\n","epoch 35 iter 22: train loss 2.55511. lr 3.829164e-03: 100% 23/23 [00:00<00:00, 25.64it/s]\n","epoch 36 iter 22: train loss 2.58272. lr 5.875354e-03: 100% 23/23 [00:00<00:00, 25.25it/s]\n","epoch 37 iter 22: train loss 2.54532. lr 2.117701e-03: 100% 23/23 [00:00<00:00, 25.57it/s]\n","epoch 38 iter 22: train loss 2.51494. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 24.62it/s]\n","epoch 39 iter 22: train loss 2.54349. lr 3.935133e-03: 100% 23/23 [00:01<00:00, 18.44it/s]\n","epoch 40 iter 22: train loss 2.57037. lr 5.841769e-03: 100% 23/23 [00:01<00:00, 20.10it/s]\n","epoch 41 iter 22: train loss 2.53929. lr 2.012353e-03: 100% 23/23 [00:01<00:00, 18.06it/s]\n","epoch 42 iter 22: train loss 2.51394. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 19.05it/s]\n","epoch 43 iter 22: train loss 2.51716. lr 4.039824e-03: 100% 23/23 [00:00<00:00, 23.37it/s]\n","epoch 44 iter 22: train loss 2.54121. lr 5.804302e-03: 100% 23/23 [00:00<00:00, 24.91it/s]\n","epoch 45 iter 22: train loss 2.52263. lr 1.908354e-03: 100% 23/23 [00:00<00:00, 25.17it/s]\n","epoch 46 iter 22: train loss 2.50593. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 26.06it/s]\n","epoch 47 iter 22: train loss 2.50325. lr 4.143096e-03: 100% 23/23 [00:00<00:00, 25.46it/s]\n","epoch 48 iter 22: train loss 2.51705. lr 5.763005e-03: 100% 23/23 [00:00<00:00, 25.08it/s]\n","epoch 49 iter 22: train loss 2.50296. lr 1.805846e-03: 100% 23/23 [00:00<00:00, 24.86it/s]\n","epoch 50 iter 22: train loss 2.47677. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 24.23it/s]\n","epoch 51 iter 22: train loss 2.50404. lr 4.244806e-03: 100% 23/23 [00:01<00:00, 19.49it/s]\n","epoch 52 iter 22: train loss 2.53155. lr 5.717935e-03: 100% 23/23 [00:01<00:00, 20.13it/s]\n","epoch 53 iter 22: train loss 2.48859. lr 1.704968e-03: 100% 23/23 [00:01<00:00, 19.66it/s]\n","epoch 54 iter 22: train loss 2.47459. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 19.27it/s]\n","epoch 55 iter 22: train loss 2.47046. lr 4.344816e-03: 100% 23/23 [00:01<00:00, 18.46it/s]\n","epoch 56 iter 22: train loss 2.50183. lr 5.669152e-03: 100% 23/23 [00:01<00:00, 18.68it/s]\n","epoch 57 iter 22: train loss 2.48657. lr 1.605860e-03: 100% 23/23 [00:00<00:00, 24.98it/s]\n","epoch 58 iter 22: train loss 2.46443. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 25.78it/s]\n","epoch 59 iter 22: train loss 2.47220. lr 4.442989e-03: 100% 23/23 [00:00<00:00, 25.11it/s]\n","epoch 60 iter 22: train loss 2.46940. lr 5.616723e-03: 100% 23/23 [00:00<00:00, 24.79it/s]\n","epoch 61 iter 22: train loss 2.46850. lr 1.508656e-03: 100% 23/23 [00:00<00:00, 25.05it/s]\n","epoch 62 iter 22: train loss 2.42108. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 25.02it/s]\n","epoch 63 iter 22: train loss 2.43965. lr 4.539191e-03: 100% 23/23 [00:00<00:00, 25.49it/s]\n","epoch 64 iter 22: train loss 2.47874. lr 5.560720e-03: 100% 23/23 [00:00<00:00, 25.69it/s]\n","epoch 65 iter 22: train loss 2.42794. lr 1.413488e-03: 100% 23/23 [00:00<00:00, 25.29it/s]\n","epoch 66 iter 22: train loss 2.40646. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 19.63it/s]\n","epoch 67 iter 22: train loss 2.44595. lr 4.633291e-03: 100% 23/23 [00:01<00:00, 19.29it/s]\n","epoch 68 iter 22: train loss 2.46961. lr 5.501220e-03: 100% 23/23 [00:01<00:00, 19.20it/s]\n","epoch 69 iter 22: train loss 2.41195. lr 1.320488e-03: 100% 23/23 [00:01<00:00, 17.97it/s]\n","epoch 70 iter 22: train loss 2.40492. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 21.99it/s]\n","epoch 71 iter 22: train loss 2.41970. lr 4.725160e-03: 100% 23/23 [00:00<00:00, 25.06it/s]\n","epoch 72 iter 22: train loss 2.43530. lr 5.438303e-03: 100% 23/23 [00:00<00:00, 25.59it/s]\n","epoch 73 iter 22: train loss 2.40429. lr 1.229782e-03: 100% 23/23 [00:00<00:00, 25.94it/s]\n","epoch 74 iter 22: train loss 2.40954. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 25.60it/s]\n","epoch 75 iter 22: train loss 2.39430. lr 4.814672e-03: 100% 23/23 [00:00<00:00, 25.44it/s]\n","epoch 76 iter 22: train loss 2.42471. lr 5.372056e-03: 100% 23/23 [00:00<00:00, 25.14it/s]\n","epoch 77 iter 22: train loss 2.38024. lr 1.141493e-03: 100% 23/23 [00:01<00:00, 19.33it/s]\n","epoch 78 iter 22: train loss 2.36229. lr 6.622880e-04: 100% 23/23 [00:00<00:00, 24.94it/s]\n","epoch 79 iter 22: train loss 2.38513. lr 4.901706e-03: 100% 23/23 [00:01<00:00, 20.50it/s]\n","epoch 80 iter 22: train loss 2.41581. lr 5.302569e-03: 100% 23/23 [00:01<00:00, 18.12it/s]\n","epoch 81 iter 22: train loss 2.38134. lr 1.055743e-03: 100% 23/23 [00:01<00:00, 18.28it/s]\n","epoch 82 iter 22: train loss 2.37073. lr 7.333594e-04: 100% 23/23 [00:01<00:00, 17.87it/s]\n","epoch 83 iter 22: train loss 2.39998. lr 4.986143e-03: 100% 23/23 [00:01<00:00, 20.66it/s]\n","epoch 84 iter 22: train loss 2.38416. lr 5.229938e-03: 100% 23/23 [00:00<00:00, 24.43it/s]\n","epoch 85 iter 22: train loss 2.35785. lr 9.726487e-04: 100% 23/23 [00:00<00:00, 25.10it/s]\n","epoch 86 iter 22: train loss 2.33067. lr 8.075268e-04: 100% 23/23 [00:00<00:00, 25.20it/s]\n","epoch 87 iter 22: train loss 2.35413. lr 5.067867e-03: 100% 23/23 [00:00<00:00, 25.69it/s]\n","epoch 88 iter 22: train loss 2.37659. lr 5.154260e-03: 100% 23/23 [00:00<00:00, 25.02it/s]\n","epoch 89 iter 22: train loss 2.36434. lr 8.923233e-04: 100% 23/23 [00:00<00:00, 25.61it/s]\n","epoch 90 iter 22: train loss 2.32484. lr 8.846887e-04: 100% 23/23 [00:00<00:00, 25.18it/s]\n","epoch 91 iter 22: train loss 2.33811. lr 5.146767e-03: 100% 23/23 [00:00<00:00, 24.76it/s]\n","epoch 92 iter 22: train loss 2.36289. lr 5.075640e-03: 100% 23/23 [00:00<00:00, 24.65it/s]\n","epoch 93 iter 22: train loss 2.30970. lr 8.148766e-04: 100% 23/23 [00:01<00:00, 18.62it/s]\n","epoch 94 iter 22: train loss 2.30960. lr 9.647399e-04: 100% 23/23 [00:01<00:00, 19.40it/s]\n","epoch 95 iter 22: train loss 2.32223. lr 5.222734e-03: 100% 23/23 [00:01<00:00, 19.37it/s]\n","epoch 96 iter 22: train loss 2.34936. lr 4.994185e-03: 100% 23/23 [00:01<00:00, 18.83it/s]\n","epoch 97 iter 22: train loss 2.34443. lr 7.404145e-04: 100% 23/23 [00:00<00:00, 23.14it/s]\n","epoch 98 iter 22: train loss 2.30132. lr 1.047571e-03: 100% 23/23 [00:00<00:00, 25.13it/s]\n","epoch 99 iter 22: train loss 2.32623. lr 5.295665e-03: 100% 23/23 [00:00<00:00, 25.34it/s]\n","epoch 100 iter 22: train loss 2.34885. lr 4.910006e-03: 100% 23/23 [00:00<00:00, 25.57it/s]\n","epoch 101 iter 22: train loss 2.30919. lr 6.690387e-04: 100% 23/23 [00:00<00:00, 24.48it/s]\n","epoch 102 iter 22: train loss 2.28921. lr 1.133069e-03: 100% 23/23 [00:00<00:00, 24.72it/s]\n","epoch 103 iter 22: train loss 2.31238. lr 5.365461e-03: 100% 23/23 [00:01<00:00, 22.27it/s]\n","epoch 104 iter 22: train loss 2.32875. lr 4.823219e-03: 100% 23/23 [00:00<00:00, 23.58it/s]\n","epoch 105 iter 22: train loss 2.29852. lr 6.008466e-04: 100% 23/23 [00:00<00:00, 25.18it/s]\n","epoch 106 iter 22: train loss 2.27298. lr 1.221116e-03: 100% 23/23 [00:01<00:00, 18.91it/s]\n","epoch 107 iter 22: train loss 2.29511. lr 5.432026e-03: 100% 23/23 [00:01<00:00, 18.72it/s]\n","epoch 108 iter 22: train loss 2.34127. lr 4.733941e-03: 100% 23/23 [00:01<00:00, 18.07it/s]\n","epoch 109 iter 22: train loss 2.27067. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 19.29it/s]\n","epoch 110 iter 22: train loss 2.28209. lr 1.311594e-03: 100% 23/23 [00:01<00:00, 21.26it/s]\n","epoch 111 iter 22: train loss 2.29102. lr 5.495270e-03: 100% 23/23 [00:00<00:00, 24.57it/s]\n","epoch 112 iter 22: train loss 2.31592. lr 4.642295e-03: 100% 23/23 [00:00<00:00, 25.19it/s]\n","epoch 113 iter 22: train loss 2.26608. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 25.46it/s]\n","epoch 114 iter 22: train loss 2.24725. lr 1.404377e-03: 100% 23/23 [00:00<00:00, 24.77it/s]\n","epoch 115 iter 22: train loss 2.28292. lr 5.555105e-03: 100% 23/23 [00:00<00:00, 25.05it/s]\n","epoch 116 iter 22: train loss 2.30715. lr 4.548406e-03: 100% 23/23 [00:00<00:00, 24.79it/s]\n","epoch 117 iter 22: train loss 2.26453. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 24.86it/s]\n","epoch 118 iter 22: train loss 2.21215. lr 1.499340e-03: 100% 23/23 [00:00<00:00, 25.27it/s]\n","epoch 119 iter 22: train loss 2.24862. lr 5.611450e-03: 100% 23/23 [00:00<00:00, 23.72it/s]\n","epoch 120 iter 22: train loss 2.29997. lr 4.452402e-03: 100% 23/23 [00:01<00:00, 18.90it/s]\n","epoch 121 iter 22: train loss 2.21298. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.95it/s]\n","epoch 122 iter 22: train loss 2.23044. lr 1.596352e-03: 100% 23/23 [00:01<00:00, 19.11it/s]\n","epoch 123 iter 22: train loss 2.28557. lr 5.664228e-03: 100% 23/23 [00:01<00:00, 18.88it/s]\n","epoch 124 iter 22: train loss 2.25253. lr 4.354414e-03: 100% 23/23 [00:01<00:00, 22.52it/s]\n","epoch 125 iter 22: train loss 2.24828. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 24.49it/s]\n","epoch 126 iter 22: train loss 2.22204. lr 1.695282e-03: 100% 23/23 [00:00<00:00, 25.02it/s]\n","epoch 127 iter 22: train loss 2.25225. lr 5.713368e-03: 100% 23/23 [00:00<00:00, 24.47it/s]\n","epoch 128 iter 22: train loss 2.23579. lr 4.254576e-03: 100% 23/23 [00:00<00:00, 24.86it/s]\n","epoch 129 iter 22: train loss 2.18970. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 24.69it/s]\n","epoch 130 iter 22: train loss 2.21240. lr 1.795994e-03: 100% 23/23 [00:01<00:00, 21.41it/s]\n","epoch 131 iter 22: train loss 2.23059. lr 5.758801e-03: 100% 23/23 [00:00<00:00, 24.96it/s]\n","epoch 132 iter 22: train loss 2.25472. lr 4.153025e-03: 100% 23/23 [00:00<00:00, 26.03it/s]\n","epoch 133 iter 22: train loss 2.22734. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 19.49it/s]\n","epoch 134 iter 22: train loss 2.19403. lr 1.898350e-03: 100% 23/23 [00:01<00:00, 18.64it/s]\n","epoch 135 iter 22: train loss 2.22611. lr 5.800466e-03: 100% 23/23 [00:01<00:00, 20.42it/s]\n","epoch 136 iter 22: train loss 2.22724. lr 4.049899e-03: 100% 23/23 [00:01<00:00, 18.66it/s]\n","epoch 137 iter 22: train loss 2.19532. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 20.62it/s]\n","epoch 138 iter 22: train loss 2.19930. lr 2.002211e-03: 100% 23/23 [00:00<00:00, 24.48it/s]\n","epoch 139 iter 22: train loss 2.20974. lr 5.838306e-03: 100% 23/23 [00:00<00:00, 25.66it/s]\n","epoch 140 iter 22: train loss 2.20268. lr 3.945338e-03: 100% 23/23 [00:00<00:00, 25.12it/s]\n","epoch 141 iter 22: train loss 2.17499. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 25.78it/s]\n","epoch 142 iter 22: train loss 2.17464. lr 2.107435e-03: 100% 23/23 [00:00<00:00, 24.62it/s]\n","epoch 143 iter 22: train loss 2.19624. lr 5.872270e-03: 100% 23/23 [00:00<00:00, 25.08it/s]\n","epoch 144 iter 22: train loss 2.22699. lr 3.839487e-03: 100% 23/23 [00:00<00:00, 24.91it/s]\n","epoch 145 iter 22: train loss 2.14617. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 25.21it/s]\n","epoch 146 iter 22: train loss 2.10632. lr 2.213878e-03: 100% 23/23 [00:00<00:00, 24.54it/s]\n","epoch 147 iter 22: train loss 2.18822. lr 5.902310e-03: 100% 23/23 [00:01<00:00, 19.03it/s]\n","epoch 148 iter 22: train loss 2.19180. lr 3.732489e-03: 100% 23/23 [00:01<00:00, 18.43it/s]\n","epoch 149 iter 22: train loss 2.17821. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.69it/s]\n","epoch 150 iter 22: train loss 2.17415. lr 2.321394e-03: 100% 23/23 [00:01<00:00, 18.53it/s]\n","epoch 151 iter 22: train loss 2.18962. lr 5.928387e-03: 100% 23/23 [00:01<00:00, 22.13it/s]\n","epoch 152 iter 22: train loss 2.15183. lr 3.624491e-03: 100% 23/23 [00:00<00:00, 25.42it/s]\n","epoch 153 iter 22: train loss 2.09796. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 25.14it/s]\n","epoch 154 iter 22: train loss 2.14267. lr 2.429838e-03: 100% 23/23 [00:00<00:00, 24.86it/s]\n","epoch 155 iter 22: train loss 2.17029. lr 5.950463e-03: 100% 23/23 [00:00<00:00, 25.00it/s]\n","epoch 156 iter 22: train loss 2.15421. lr 3.515639e-03: 100% 23/23 [00:01<00:00, 18.97it/s]\n","epoch 157 iter 22: train loss 2.11893. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 24.63it/s]\n","epoch 158 iter 22: train loss 2.11353. lr 2.539060e-03: 100% 23/23 [00:00<00:00, 25.15it/s]\n","epoch 159 iter 22: train loss 2.17337. lr 5.968510e-03: 100% 23/23 [00:00<00:00, 25.06it/s]\n","epoch 160 iter 22: train loss 2.13701. lr 3.406083e-03: 100% 23/23 [00:01<00:00, 20.48it/s]\n","epoch 161 iter 22: train loss 2.09268. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 19.35it/s]\n","epoch 162 iter 22: train loss 2.10785. lr 2.648912e-03: 100% 23/23 [00:01<00:00, 18.48it/s]\n","epoch 163 iter 22: train loss 2.17747. lr 5.982502e-03: 100% 23/23 [00:01<00:00, 18.64it/s]\n","epoch 164 iter 22: train loss 2.13618. lr 3.295973e-03: 100% 23/23 [00:01<00:00, 20.31it/s]\n","epoch 165 iter 22: train loss 2.08709. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 25.42it/s]\n","epoch 166 iter 22: train loss 2.08460. lr 2.759243e-03: 100% 23/23 [00:00<00:00, 25.25it/s]\n","epoch 167 iter 22: train loss 2.16393. lr 5.992421e-03: 100% 23/23 [00:00<00:00, 24.84it/s]\n","epoch 168 iter 22: train loss 2.12632. lr 3.185458e-03: 100% 23/23 [00:00<00:00, 25.10it/s]\n","epoch 169 iter 22: train loss 2.08961. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 24.84it/s]\n","epoch 170 iter 22: train loss 2.02351. lr 2.869903e-03: 100% 23/23 [00:00<00:00, 25.04it/s]\n","epoch 171 iter 22: train loss 2.14352. lr 5.998252e-03: 100% 23/23 [00:00<00:00, 25.90it/s]\n","epoch 172 iter 22: train loss 2.07728. lr 3.074690e-03: 100% 23/23 [00:00<00:00, 25.58it/s]\n","epoch 173 iter 22: train loss 2.02277. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 24.82it/s]\n","epoch 174 iter 22: train loss 2.03184. lr 2.980741e-03: 100% 23/23 [00:01<00:00, 19.31it/s]\n","epoch 175 iter 22: train loss 2.10738. lr 5.999988e-03: 100% 23/23 [00:01<00:00, 17.81it/s]\n","epoch 176 iter 22: train loss 2.05941. lr 2.963821e-03: 100% 23/23 [00:01<00:00, 18.99it/s]\n","epoch 177 iter 22: train loss 2.02839. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.89it/s]\n","epoch 178 iter 22: train loss 2.04781. lr 3.091605e-03: 100% 23/23 [00:01<00:00, 21.22it/s]\n","epoch 179 iter 22: train loss 2.07096. lr 5.997627e-03: 100% 23/23 [00:00<00:00, 25.26it/s]\n","epoch 180 iter 22: train loss 2.07362. lr 2.853000e-03: 100% 23/23 [00:00<00:00, 24.49it/s]\n","epoch 181 iter 22: train loss 1.99036. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 25.03it/s]\n","epoch 182 iter 22: train loss 2.00546. lr 3.202344e-03: 100% 23/23 [00:01<00:00, 19.96it/s]\n","epoch 183 iter 22: train loss 2.06520. lr 5.991171e-03: 100% 23/23 [00:00<00:00, 25.34it/s]\n","epoch 184 iter 22: train loss 2.03020. lr 2.742380e-03: 100% 23/23 [00:00<00:00, 24.76it/s]\n","epoch 185 iter 22: train loss 1.98162. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 25.51it/s]\n","epoch 186 iter 22: train loss 1.94902. lr 3.312807e-03: 100% 23/23 [00:00<00:00, 25.57it/s]\n","epoch 187 iter 22: train loss 2.06142. lr 5.980630e-03: 100% 23/23 [00:01<00:00, 22.13it/s]\n","epoch 188 iter 22: train loss 2.01182. lr 2.632112e-03: 100% 23/23 [00:01<00:00, 18.22it/s]\n","epoch 189 iter 22: train loss 1.93284. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.24it/s]\n","epoch 190 iter 22: train loss 1.94901. lr 3.422843e-03: 100% 23/23 [00:01<00:00, 19.80it/s]\n","epoch 191 iter 22: train loss 1.98904. lr 5.966017e-03: 100% 23/23 [00:01<00:00, 19.57it/s]\n","epoch 192 iter 22: train loss 1.96561. lr 2.522347e-03: 100% 23/23 [00:00<00:00, 24.83it/s]\n","epoch 193 iter 22: train loss 1.90050. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 25.46it/s]\n","epoch 194 iter 22: train loss 1.93836. lr 3.532300e-03: 100% 23/23 [00:00<00:00, 25.50it/s]\n","epoch 195 iter 22: train loss 1.98506. lr 5.947354e-03: 100% 23/23 [00:00<00:00, 25.09it/s]\n","epoch 196 iter 22: train loss 1.93714. lr 2.413234e-03: 100% 23/23 [00:00<00:00, 25.75it/s]\n","epoch 197 iter 22: train loss 1.91092. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 25.23it/s]\n","epoch 198 iter 22: train loss 1.90731. lr 3.641031e-03: 100% 23/23 [00:00<00:00, 24.72it/s]\n","epoch 199 iter 22: train loss 1.99147. lr 5.924665e-03: 100% 23/23 [00:00<00:00, 24.63it/s]\n","epoch 200 iter 22: train loss 1.92367. lr 2.304922e-03: 100% 23/23 [00:00<00:00, 24.69it/s]\n","epoch 201 iter 22: train loss 1.85512. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 19.37it/s]\n","epoch 202 iter 22: train loss 1.88907. lr 3.748887e-03: 100% 23/23 [00:01<00:00, 18.94it/s]\n","epoch 203 iter 22: train loss 1.95733. lr 5.897981e-03: 100% 23/23 [00:01<00:00, 18.47it/s]\n","epoch 204 iter 22: train loss 1.90981. lr 2.197560e-03: 100% 23/23 [00:01<00:00, 18.54it/s]\n","epoch 205 iter 22: train loss 1.85977. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 21.05it/s]\n","epoch 206 iter 22: train loss 1.86499. lr 3.855719e-03: 100% 23/23 [00:00<00:00, 24.09it/s]\n","epoch 207 iter 22: train loss 1.95494. lr 5.867339e-03: 100% 23/23 [00:00<00:00, 24.42it/s]\n","epoch 208 iter 22: train loss 1.93041. lr 2.091294e-03: 100% 23/23 [00:00<00:00, 24.74it/s]\n","epoch 209 iter 22: train loss 1.84272. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 20.75it/s]\n","epoch 210 iter 22: train loss 1.82515. lr 3.961383e-03: 100% 23/23 [00:00<00:00, 25.28it/s]\n","epoch 211 iter 22: train loss 1.95806. lr 5.832781e-03: 100% 23/23 [00:00<00:00, 24.91it/s]\n","epoch 212 iter 22: train loss 1.85891. lr 1.986269e-03: 100% 23/23 [00:00<00:00, 25.07it/s]\n","epoch 213 iter 22: train loss 1.85660. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 24.55it/s]\n","epoch 214 iter 22: train loss 1.82346. lr 4.065733e-03: 100% 23/23 [00:01<00:00, 20.62it/s]\n","epoch 215 iter 22: train loss 1.89088. lr 5.794354e-03: 100% 23/23 [00:01<00:00, 19.25it/s]\n","epoch 216 iter 22: train loss 1.81520. lr 1.882629e-03: 100% 23/23 [00:01<00:00, 18.40it/s]\n","epoch 217 iter 22: train loss 1.76249. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.62it/s]\n","epoch 218 iter 22: train loss 1.76916. lr 4.168628e-03: 100% 23/23 [00:01<00:00, 20.28it/s]\n","epoch 219 iter 22: train loss 1.84985. lr 5.752109e-03: 100% 23/23 [00:00<00:00, 25.67it/s]\n","epoch 220 iter 22: train loss 1.80827. lr 1.780514e-03: 100% 23/23 [00:00<00:00, 25.06it/s]\n","epoch 221 iter 22: train loss 1.73688. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 25.20it/s]\n","epoch 222 iter 22: train loss 1.78971. lr 4.269927e-03: 100% 23/23 [00:00<00:00, 24.80it/s]\n","epoch 223 iter 22: train loss 1.86120. lr 5.706106e-03: 100% 23/23 [00:00<00:00, 25.21it/s]\n","epoch 224 iter 22: train loss 1.78496. lr 1.680066e-03: 100% 23/23 [00:00<00:00, 24.93it/s]\n","epoch 225 iter 22: train loss 1.71195. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 22.64it/s]\n","epoch 226 iter 22: train loss 1.75868. lr 4.369491e-03: 100% 23/23 [00:00<00:00, 25.32it/s]\n","epoch 227 iter 22: train loss 1.81418. lr 5.656407e-03: 100% 23/23 [00:00<00:00, 23.84it/s]\n","epoch 228 iter 22: train loss 1.78371. lr 1.581420e-03: 100% 23/23 [00:01<00:00, 19.20it/s]\n","epoch 229 iter 22: train loss 1.70768. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.14it/s]\n","epoch 230 iter 22: train loss 1.76083. lr 4.467185e-03: 100% 23/23 [00:01<00:00, 18.78it/s]\n","epoch 231 iter 22: train loss 1.83751. lr 5.603080e-03: 100% 23/23 [00:01<00:00, 18.86it/s]\n","epoch 232 iter 22: train loss 1.76063. lr 1.484712e-03: 100% 23/23 [00:01<00:00, 22.78it/s]\n","epoch 233 iter 22: train loss 1.71010. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 25.29it/s]\n","epoch 234 iter 22: train loss 1.74943. lr 4.562874e-03: 100% 23/23 [00:00<00:00, 25.10it/s]\n","epoch 235 iter 22: train loss 1.80633. lr 5.546197e-03: 100% 23/23 [00:00<00:00, 23.36it/s]\n","epoch 236 iter 22: train loss 1.77704. lr 1.390073e-03: 100% 23/23 [00:00<00:00, 24.78it/s]\n","epoch 237 iter 22: train loss 1.67674. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 25.28it/s]\n","epoch 238 iter 22: train loss 1.70636. lr 4.656430e-03: 100% 23/23 [00:00<00:00, 24.73it/s]\n","epoch 239 iter 22: train loss 1.79709. lr 5.485836e-03: 100% 23/23 [00:00<00:00, 25.25it/s]\n","epoch 240 iter 22: train loss 1.76800. lr 1.297633e-03: 100% 23/23 [00:00<00:00, 24.18it/s]\n","epoch 241 iter 22: train loss 1.63937. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 19.57it/s]\n","epoch 242 iter 22: train loss 1.68798. lr 4.747722e-03: 100% 23/23 [00:01<00:00, 18.59it/s]\n","epoch 243 iter 22: train loss 1.76203. lr 5.422080e-03: 100% 23/23 [00:01<00:00, 18.74it/s]\n","epoch 244 iter 22: train loss 1.72224. lr 1.207519e-03: 100% 23/23 [00:01<00:00, 18.79it/s]\n","epoch 245 iter 22: train loss 1.65551. lr 6.110439e-04: 100% 23/23 [00:01<00:00, 20.57it/s]\n","epoch 246 iter 22: train loss 1.68806. lr 4.836628e-03: 100% 23/23 [00:00<00:00, 25.11it/s]\n","epoch 247 iter 22: train loss 1.82150. lr 5.355016e-03: 100% 23/23 [00:00<00:00, 24.39it/s]\n","epoch 248 iter 22: train loss 1.73355. lr 1.119853e-03: 100% 23/23 [00:00<00:00, 24.88it/s]\n","epoch 249 iter 22: train loss 1.63200. lr 6.797280e-04: 100% 23/23 [00:01<00:00, 20.11it/s]\n","epoch 250 iter 22: train loss 1.66637. lr 4.923025e-03: 100% 23/23 [00:00<00:00, 25.12it/s]\n","epoch 251 iter 22: train loss 1.79011. lr 5.284735e-03: 100% 23/23 [00:00<00:00, 25.17it/s]\n","epoch 252 iter 22: train loss 1.62859. lr 1.034755e-03: 100% 23/23 [00:00<00:00, 25.46it/s]\n","epoch 253 iter 22: train loss 1.65255. lr 7.515813e-04: 100% 23/23 [00:00<00:00, 25.19it/s]\n","epoch 254 iter 22: train loss 1.64505. lr 5.006795e-03: 100% 23/23 [00:01<00:00, 22.69it/s]\n","epoch 255 iter 22: train loss 1.76439. lr 5.211334e-03: 100% 23/23 [00:01<00:00, 18.68it/s]\n","epoch 256 iter 22: train loss 1.68056. lr 9.523406e-04: 100% 23/23 [00:01<00:00, 18.73it/s]\n","epoch 257 iter 22: train loss 1.61167. lr 8.265056e-04: 100% 23/23 [00:01<00:00, 18.27it/s]\n","epoch 258 iter 22: train loss 1.66203. lr 5.087825e-03: 100% 23/23 [00:01<00:00, 18.71it/s]\n","epoch 259 iter 22: train loss 1.70670. lr 5.134913e-03: 100% 23/23 [00:00<00:00, 24.92it/s]\n","epoch 260 iter 22: train loss 1.60585. lr 8.727234e-04: 100% 23/23 [00:00<00:00, 24.46it/s]\n","epoch 261 iter 22: train loss 1.59623. lr 9.043985e-04: 100% 23/23 [00:01<00:00, 19.35it/s]\n","epoch 262 iter 22: train loss 1.62661. lr 5.166002e-03: 100% 23/23 [00:00<00:00, 24.50it/s]\n","epoch 263 iter 22: train loss 1.72560. lr 5.055575e-03: 100% 23/23 [00:00<00:00, 25.07it/s]\n","epoch 264 iter 22: train loss 1.58361. lr 7.960117e-04: 100% 23/23 [00:00<00:00, 24.93it/s]\n","epoch 265 iter 22: train loss 1.62281. lr 9.851537e-04: 100% 23/23 [00:00<00:00, 24.60it/s]\n","epoch 266 iter 22: train loss 1.65040. lr 5.241222e-03: 100% 23/23 [00:00<00:00, 25.17it/s]\n","epoch 267 iter 22: train loss 1.71275. lr 4.973430e-03: 100% 23/23 [00:00<00:00, 23.85it/s]\n","epoch 268 iter 22: train loss 1.58697. lr 7.223104e-04: 100% 23/23 [00:01<00:00, 18.49it/s]\n","epoch 269 iter 22: train loss 1.54044. lr 1.068661e-03: 100% 23/23 [00:01<00:00, 19.14it/s]\n","epoch 270 iter 22: train loss 1.65292. lr 5.313380e-03: 100% 23/23 [00:01<00:00, 18.47it/s]\n","epoch 271 iter 22: train loss 1.71062. lr 4.888589e-03: 100% 23/23 [00:01<00:00, 18.85it/s]\n","epoch 272 iter 22: train loss 1.57668. lr 6.517201e-04: 100% 23/23 [00:00<00:00, 25.03it/s]\n","epoch 273 iter 22: train loss 1.52157. lr 1.154806e-03: 100% 23/23 [00:00<00:00, 23.16it/s]\n","epoch 274 iter 22: train loss 1.56974. lr 5.382378e-03: 100% 23/23 [00:00<00:00, 25.77it/s]\n","epoch 275 iter 22: train loss 1.64892. lr 4.801169e-03: 100% 23/23 [00:00<00:00, 24.74it/s]\n","epoch 276 iter 22: train loss 1.55155. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 25.28it/s]\n","epoch 277 iter 22: train loss 1.50319. lr 1.243471e-03: 100% 23/23 [00:00<00:00, 25.39it/s]\n","epoch 278 iter 22: train loss 1.59410. lr 5.448123e-03: 100% 23/23 [00:00<00:00, 25.47it/s]\n","epoch 279 iter 22: train loss 1.66587. lr 4.711289e-03: 100% 23/23 [00:00<00:00, 24.95it/s]\n","epoch 280 iter 22: train loss 1.52451. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 24.55it/s]\n","epoch 281 iter 22: train loss 1.51840. lr 1.334536e-03: 100% 23/23 [00:01<00:00, 19.42it/s]\n","epoch 282 iter 22: train loss 1.64902. lr 5.510523e-03: 100% 23/23 [00:01<00:00, 18.73it/s]\n","epoch 283 iter 22: train loss 1.65589. lr 4.619071e-03: 100% 23/23 [00:01<00:00, 18.88it/s]\n","epoch 284 iter 22: train loss 1.59063. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 19.25it/s]\n","epoch 285 iter 22: train loss 1.54776. lr 1.427875e-03: 100% 23/23 [00:01<00:00, 21.36it/s]\n","epoch 286 iter 22: train loss 1.58198. lr 5.569495e-03: 100% 23/23 [00:00<00:00, 23.66it/s]\n","epoch 287 iter 22: train loss 1.62800. lr 4.524642e-03: 100% 23/23 [00:01<00:00, 20.34it/s]\n","epoch 288 iter 22: train loss 1.46378. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 24.99it/s]\n","epoch 289 iter 22: train loss 1.43419. lr 1.523362e-03: 100% 23/23 [00:00<00:00, 24.55it/s]\n","epoch 290 iter 22: train loss 1.58734. lr 5.624957e-03: 100% 23/23 [00:00<00:00, 24.58it/s]\n","epoch 291 iter 22: train loss 1.60070. lr 4.428130e-03: 100% 23/23 [00:00<00:00, 25.13it/s]\n","epoch 292 iter 22: train loss 1.45457. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 25.35it/s]\n","epoch 293 iter 22: train loss 1.48168. lr 1.620865e-03: 100% 23/23 [00:00<00:00, 25.42it/s]\n","epoch 294 iter 22: train loss 1.60584. lr 5.676834e-03: 100% 23/23 [00:01<00:00, 20.54it/s]\n","epoch 295 iter 22: train loss 1.57005. lr 4.329668e-03: 100% 23/23 [00:01<00:00, 18.80it/s]\n","epoch 296 iter 22: train loss 1.46773. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 19.10it/s]\n","epoch 297 iter 22: train loss 1.45730. lr 1.720252e-03: 100% 23/23 [00:01<00:00, 18.70it/s]\n","epoch 298 iter 22: train loss 1.50301. lr 5.725055e-03: 100% 23/23 [00:01<00:00, 19.38it/s]\n","epoch 299 iter 22: train loss 1.57463. lr 4.229390e-03: 100% 23/23 [00:00<00:00, 24.63it/s]\n","epoch 300 iter 22: train loss 1.45894. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 25.26it/s]\n","epoch 301 iter 22: train loss 1.47759. lr 1.821388e-03: 100% 23/23 [00:00<00:00, 24.53it/s]\n","epoch 302 iter 22: train loss 1.53964. lr 5.769553e-03: 100% 23/23 [00:00<00:00, 25.38it/s]\n","epoch 303 iter 22: train loss 1.57169. lr 4.127433e-03: 100% 23/23 [00:00<00:00, 25.13it/s]\n","epoch 304 iter 22: train loss 1.52207. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 25.08it/s]\n","epoch 305 iter 22: train loss 1.43029. lr 1.924132e-03: 100% 23/23 [00:00<00:00, 24.92it/s]\n","epoch 306 iter 22: train loss 1.55667. lr 5.810269e-03: 100% 23/23 [00:00<00:00, 25.42it/s]\n","epoch 307 iter 22: train loss 1.55693. lr 4.023935e-03: 100% 23/23 [00:00<00:00, 24.98it/s]\n","epoch 308 iter 22: train loss 1.49498. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 19.60it/s]\n","epoch 309 iter 22: train loss 1.45996. lr 2.028347e-03: 100% 23/23 [00:01<00:00, 18.45it/s]\n","epoch 310 iter 22: train loss 1.50232. lr 5.847147e-03: 100% 23/23 [00:01<00:00, 19.03it/s]\n","epoch 311 iter 22: train loss 1.49855. lr 3.919039e-03: 100% 23/23 [00:01<00:00, 18.54it/s]\n","epoch 312 iter 22: train loss 1.43188. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.17it/s]\n","epoch 313 iter 22: train loss 1.44879. lr 2.133888e-03: 100% 23/23 [00:00<00:00, 24.45it/s]\n","epoch 314 iter 22: train loss 1.53246. lr 5.880135e-03: 100% 23/23 [00:00<00:00, 25.15it/s]\n","epoch 315 iter 22: train loss 1.55876. lr 3.812888e-03: 100% 23/23 [00:00<00:00, 25.45it/s]\n","epoch 316 iter 22: train loss 1.47910. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 25.34it/s]\n","epoch 317 iter 22: train loss 1.46613. lr 2.240613e-03: 100% 23/23 [00:00<00:00, 24.52it/s]\n","epoch 318 iter 22: train loss 1.56029. lr 5.909190e-03: 100% 23/23 [00:00<00:00, 24.28it/s]\n","epoch 319 iter 22: train loss 1.49233. lr 3.705627e-03: 100% 23/23 [00:00<00:00, 25.18it/s]\n","epoch 320 iter 22: train loss 1.47326. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 25.04it/s]\n","epoch 321 iter 22: train loss 1.41968. lr 2.348374e-03: 100% 23/23 [00:01<00:00, 21.60it/s]\n","epoch 322 iter 22: train loss 1.55825. lr 5.934272e-03: 100% 23/23 [00:01<00:00, 18.41it/s]\n","epoch 323 iter 22: train loss 1.57302. lr 3.597402e-03: 100% 23/23 [00:01<00:00, 19.24it/s]\n","epoch 324 iter 22: train loss 1.41152. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.09it/s]\n","epoch 325 iter 22: train loss 1.39049. lr 2.457026e-03: 100% 23/23 [00:01<00:00, 18.77it/s]\n","epoch 326 iter 22: train loss 1.46854. lr 5.955345e-03: 100% 23/23 [00:00<00:00, 24.46it/s]\n","epoch 327 iter 22: train loss 1.53120. lr 3.488361e-03: 100% 23/23 [00:00<00:00, 23.69it/s]\n","epoch 328 iter 22: train loss 1.40057. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 23.99it/s]\n","epoch 329 iter 22: train loss 1.38061. lr 2.566420e-03: 100% 23/23 [00:01<00:00, 18.85it/s]\n","epoch 330 iter 22: train loss 1.52662. lr 5.972382e-03: 100% 23/23 [00:01<00:00, 18.84it/s]\n","epoch 331 iter 22: train loss 1.53701. lr 3.378652e-03: 100% 23/23 [00:01<00:00, 18.39it/s]\n","epoch 332 iter 22: train loss 1.40369. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.16it/s]\n","epoch 333 iter 22: train loss 1.42555. lr 2.676405e-03: 100% 23/23 [00:01<00:00, 18.01it/s]\n","epoch 334 iter 22: train loss 1.46153. lr 5.985359e-03: 100% 23/23 [00:01<00:00, 17.41it/s]\n","epoch 335 iter 22: train loss 1.49430. lr 3.268427e-03: 100% 23/23 [00:01<00:00, 18.73it/s]\n","epoch 336 iter 22: train loss 1.41767. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 16.66it/s]\n","epoch 337 iter 22: train loss 1.31982. lr 2.786833e-03: 100% 23/23 [00:01<00:00, 19.16it/s]\n","epoch 338 iter 22: train loss 1.51419. lr 5.994259e-03: 100% 23/23 [00:01<00:00, 20.87it/s]\n","epoch 339 iter 22: train loss 1.48316. lr 3.157835e-03: 100% 23/23 [00:00<00:00, 25.44it/s]\n","epoch 340 iter 22: train loss 1.36862. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 25.22it/s]\n","epoch 341 iter 22: train loss 1.38477. lr 2.897551e-03: 100% 23/23 [00:00<00:00, 24.94it/s]\n","epoch 342 iter 22: train loss 1.55458. lr 5.999069e-03: 100% 23/23 [00:00<00:00, 25.82it/s]\n","epoch 343 iter 22: train loss 1.51405. lr 3.047027e-03: 100% 23/23 [00:00<00:00, 25.39it/s]\n","epoch 344 iter 22: train loss 1.37239. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 24.57it/s]\n","epoch 345 iter 22: train loss 1.36805. lr 3.008410e-03: 100% 23/23 [00:00<00:00, 23.42it/s]\n","epoch 346 iter 22: train loss 1.49054. lr 5.999782e-03: 100% 23/23 [00:00<00:00, 24.95it/s]\n","epoch 347 iter 22: train loss 1.47470. lr 2.936156e-03: 100% 23/23 [00:00<00:00, 23.43it/s]\n","epoch 348 iter 22: train loss 1.33745. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.56it/s]\n","epoch 349 iter 22: train loss 1.36508. lr 3.119257e-03: 100% 23/23 [00:01<00:00, 18.93it/s]\n","epoch 350 iter 22: train loss 1.50089. lr 5.996399e-03: 100% 23/23 [00:01<00:00, 18.36it/s]\n","epoch 351 iter 22: train loss 1.45277. lr 2.825371e-03: 100% 23/23 [00:01<00:00, 19.13it/s]\n","epoch 352 iter 22: train loss 1.31645. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 24.00it/s]\n","epoch 353 iter 22: train loss 1.31049. lr 3.229941e-03: 100% 23/23 [00:00<00:00, 25.14it/s]\n","epoch 354 iter 22: train loss 1.45732. lr 5.988923e-03: 100% 23/23 [00:00<00:00, 25.37it/s]\n","epoch 355 iter 22: train loss 1.48971. lr 2.714825e-03: 100% 23/23 [00:00<00:00, 25.42it/s]\n","epoch 356 iter 22: train loss 1.35388. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 24.79it/s]\n","epoch 357 iter 22: train loss 1.35131. lr 3.340311e-03: 100% 23/23 [00:00<00:00, 25.58it/s]\n","epoch 358 iter 22: train loss 1.45580. lr 5.977364e-03: 100% 23/23 [00:00<00:00, 24.73it/s]\n","epoch 359 iter 22: train loss 1.40702. lr 2.604668e-03: 100% 23/23 [00:00<00:00, 25.75it/s]\n","epoch 360 iter 22: train loss 1.35073. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 26.27it/s]\n","epoch 361 iter 22: train loss 1.35731. lr 3.450217e-03: 100% 23/23 [00:01<00:00, 17.88it/s]\n","epoch 362 iter 22: train loss 1.48422. lr 5.961739e-03: 100% 23/23 [00:01<00:00, 19.03it/s]\n","epoch 363 iter 22: train loss 1.40187. lr 2.495052e-03: 100% 23/23 [00:01<00:00, 16.05it/s]\n","epoch 364 iter 22: train loss 1.30334. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 17.62it/s]\n","epoch 365 iter 22: train loss 1.36989. lr 3.559507e-03: 100% 23/23 [00:00<00:00, 24.10it/s]\n","epoch 366 iter 22: train loss 1.45678. lr 5.942068e-03: 100% 23/23 [00:00<00:00, 25.02it/s]\n","epoch 367 iter 22: train loss 1.43231. lr 2.386125e-03: 100% 23/23 [00:00<00:00, 25.05it/s]\n","epoch 368 iter 22: train loss 1.27931. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 25.13it/s]\n","epoch 369 iter 22: train loss 1.33899. lr 3.668033e-03: 100% 23/23 [00:00<00:00, 23.67it/s]\n","epoch 370 iter 22: train loss 1.50703. lr 5.918379e-03: 100% 23/23 [00:00<00:00, 25.02it/s]\n","epoch 371 iter 22: train loss 1.40087. lr 2.278036e-03: 100% 23/23 [00:00<00:00, 25.48it/s]\n","epoch 372 iter 22: train loss 1.30579. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 25.46it/s]\n","epoch 373 iter 22: train loss 1.32165. lr 3.775647e-03: 100% 23/23 [00:00<00:00, 25.60it/s]\n","epoch 374 iter 22: train loss 1.46997. lr 5.890704e-03: 100% 23/23 [00:01<00:00, 19.28it/s]\n","epoch 375 iter 22: train loss 1.42120. lr 2.170934e-03: 100% 23/23 [00:01<00:00, 19.21it/s]\n","epoch 376 iter 22: train loss 1.33999. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.69it/s]\n","epoch 377 iter 22: train loss 1.29853. lr 3.882202e-03: 100% 23/23 [00:01<00:00, 18.99it/s]\n","epoch 378 iter 22: train loss 1.45428. lr 5.859081e-03: 100% 23/23 [00:01<00:00, 20.01it/s]\n","epoch 379 iter 22: train loss 1.39659. lr 2.064964e-03: 100% 23/23 [00:00<00:00, 25.09it/s]\n","epoch 380 iter 22: train loss 1.31577. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 25.60it/s]\n","epoch 381 iter 22: train loss 1.29996. lr 3.987551e-03: 100% 23/23 [00:00<00:00, 25.06it/s]\n","epoch 382 iter 22: train loss 1.43635. lr 5.823552e-03: 100% 23/23 [00:00<00:00, 24.86it/s]\n","epoch 383 iter 22: train loss 1.42380. lr 1.960271e-03: 100% 23/23 [00:00<00:00, 24.90it/s]\n","epoch 384 iter 22: train loss 1.32241. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 25.14it/s]\n","epoch 385 iter 22: train loss 1.28696. lr 4.091552e-03: 100% 23/23 [00:00<00:00, 24.84it/s]\n","epoch 386 iter 22: train loss 1.42877. lr 5.784167e-03: 100% 23/23 [00:00<00:00, 25.38it/s]\n","epoch 387 iter 22: train loss 1.35084. lr 1.856998e-03: 100% 23/23 [00:00<00:00, 23.98it/s]\n","epoch 388 iter 22: train loss 1.29992. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.92it/s]\n","epoch 389 iter 22: train loss 1.34174. lr 4.194061e-03: 100% 23/23 [00:01<00:00, 15.94it/s]\n","epoch 390 iter 22: train loss 1.43046. lr 5.740979e-03: 100% 23/23 [00:01<00:00, 18.74it/s]\n","epoch 391 iter 22: train loss 1.39372. lr 1.755287e-03: 100% 23/23 [00:01<00:00, 18.40it/s]\n","epoch 392 iter 22: train loss 1.32467. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 23.59it/s]\n","epoch 393 iter 22: train loss 1.30286. lr 4.294940e-03: 100% 23/23 [00:00<00:00, 24.69it/s]\n","epoch 394 iter 22: train loss 1.38770. lr 5.694048e-03: 100% 23/23 [00:00<00:00, 24.43it/s]\n","epoch 395 iter 22: train loss 1.37132. lr 1.655275e-03: 100% 23/23 [00:00<00:00, 24.31it/s]\n","epoch 396 iter 22: train loss 1.27322. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 26.07it/s]\n","epoch 397 iter 22: train loss 1.31465. lr 4.394050e-03: 100% 23/23 [00:00<00:00, 25.28it/s]\n","epoch 398 iter 22: train loss 1.39282. lr 5.643437e-03: 100% 23/23 [00:00<00:00, 24.79it/s]\n","epoch 399 iter 22: train loss 1.35867. lr 1.557101e-03: 100% 23/23 [00:00<00:00, 25.69it/s]\n","epoch 400 iter 22: train loss 1.29516. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 25.72it/s]\n","epoch 401 iter 22: train loss 1.26782. lr 4.491256e-03: 100% 23/23 [00:01<00:00, 20.46it/s]\n","epoch 402 iter 22: train loss 1.41933. lr 5.589215e-03: 100% 23/23 [00:01<00:00, 19.24it/s]\n","epoch 403 iter 22: train loss 1.43926. lr 1.460897e-03: 100% 23/23 [00:01<00:00, 18.87it/s]\n","epoch 404 iter 22: train loss 1.29124. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.64it/s]\n","epoch 405 iter 22: train loss 1.29654. lr 4.586425e-03: 100% 23/23 [00:01<00:00, 19.68it/s]\n","epoch 406 iter 22: train loss 1.44509. lr 5.531457e-03: 100% 23/23 [00:00<00:00, 23.86it/s]\n","epoch 407 iter 22: train loss 1.34167. lr 1.366795e-03: 100% 23/23 [00:00<00:00, 24.14it/s]\n","epoch 408 iter 22: train loss 1.30155. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 25.02it/s]\n","epoch 409 iter 22: train loss 1.30428. lr 4.679428e-03: 100% 23/23 [00:00<00:00, 24.53it/s]\n","epoch 410 iter 22: train loss 1.42394. lr 5.470241e-03: 100% 23/23 [00:01<00:00, 22.90it/s]\n","epoch 411 iter 22: train loss 1.30437. lr 1.274924e-03: 100% 23/23 [00:01<00:00, 22.15it/s]\n","epoch 412 iter 22: train loss 1.32275. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 25.01it/s]\n","epoch 413 iter 22: train loss 1.33713. lr 4.770136e-03: 100% 23/23 [00:00<00:00, 25.08it/s]\n","epoch 414 iter 22: train loss 1.34035. lr 5.405651e-03: 100% 23/23 [00:00<00:00, 23.57it/s]\n","epoch 415 iter 22: train loss 1.32117. lr 1.185409e-03: 100% 23/23 [00:01<00:00, 14.11it/s]\n","epoch 416 iter 22: train loss 1.28429. lr 6.278815e-04: 100% 23/23 [00:01<00:00, 17.01it/s]\n","epoch 417 iter 22: train loss 1.29715. lr 4.858427e-03: 100% 23/23 [00:01<00:00, 19.60it/s]\n","epoch 418 iter 22: train loss 1.39329. lr 5.337776e-03: 100% 23/23 [00:01<00:00, 18.60it/s]\n","epoch 419 iter 22: train loss 1.34342. lr 1.098372e-03: 100% 23/23 [00:00<00:00, 25.23it/s]\n","epoch 420 iter 22: train loss 1.30735. lr 6.973654e-04: 100% 23/23 [00:00<00:00, 24.79it/s]\n","epoch 421 iter 22: train loss 1.33279. lr 4.944179e-03: 100% 23/23 [00:00<00:00, 25.88it/s]\n","epoch 422 iter 22: train loss 1.43037. lr 5.266707e-03: 100% 23/23 [00:00<00:00, 25.31it/s]\n","epoch 423 iter 22: train loss 1.27445. lr 1.013933e-03: 100% 23/23 [00:00<00:00, 25.10it/s]\n","epoch 424 iter 22: train loss 1.23509. lr 7.699944e-04: 100% 23/23 [00:00<00:00, 24.94it/s]\n","epoch 425 iter 22: train loss 1.35312. lr 5.027276e-03: 100% 23/23 [00:00<00:00, 25.15it/s]\n","epoch 426 iter 22: train loss 1.40912. lr 5.192543e-03: 100% 23/23 [00:00<00:00, 24.51it/s]\n","epoch 427 iter 22: train loss 1.30744. lr 9.322066e-04: 100% 23/23 [00:00<00:00, 24.95it/s]\n","epoch 428 iter 22: train loss 1.28959. lr 8.456692e-04: 100% 23/23 [00:01<00:00, 18.90it/s]\n","epoch 429 iter 22: train loss 1.34388. lr 5.107604e-03: 100% 23/23 [00:01<00:00, 18.34it/s]\n","epoch 430 iter 22: train loss 1.42874. lr 5.115383e-03: 100% 23/23 [00:01<00:00, 18.60it/s]\n","epoch 431 iter 22: train loss 1.30039. lr 8.533044e-04: 100% 23/23 [00:01<00:00, 18.68it/s]\n","epoch 432 iter 22: train loss 1.23757. lr 9.242865e-04: 100% 23/23 [00:01<00:00, 20.80it/s]\n","epoch 433 iter 22: train loss 1.31377. lr 5.185054e-03: 100% 23/23 [00:00<00:00, 24.50it/s]\n","epoch 434 iter 22: train loss 1.47004. lr 5.035335e-03: 100% 23/23 [00:00<00:00, 25.37it/s]\n","epoch 435 iter 22: train loss 1.31158. lr 7.773343e-04: 100% 23/23 [00:00<00:00, 24.97it/s]\n","epoch 436 iter 22: train loss 1.25064. lr 1.005739e-03: 100% 23/23 [00:00<00:00, 24.18it/s]\n","epoch 437 iter 22: train loss 1.35598. lr 5.259519e-03: 100% 23/23 [00:00<00:00, 25.17it/s]\n","epoch 438 iter 22: train loss 1.43367. lr 4.952506e-03: 100% 23/23 [00:00<00:00, 24.84it/s]\n","epoch 439 iter 22: train loss 1.30772. lr 7.044001e-04: 100% 23/23 [00:00<00:00, 24.92it/s]\n","epoch 440 iter 22: train loss 1.26881. lr 1.089915e-03: 100% 23/23 [00:00<00:00, 24.45it/s]\n","epoch 441 iter 22: train loss 1.33442. lr 5.330897e-03: 100% 23/23 [00:01<00:00, 16.81it/s]\n","epoch 442 iter 22: train loss 1.40046. lr 4.867011e-03: 100% 23/23 [00:01<00:00, 18.26it/s]\n","epoch 443 iter 22: train loss 1.28225. lr 6.346012e-04: 100% 23/23 [00:01<00:00, 18.32it/s]\n","epoch 444 iter 22: train loss 1.27393. lr 1.176700e-03: 100% 23/23 [00:01<00:00, 18.79it/s]\n","epoch 445 iter 22: train loss 1.32916. lr 5.399092e-03: 100% 23/23 [00:01<00:00, 20.07it/s]\n","epoch 446 iter 22: train loss 1.37547. lr 4.778966e-03: 100% 23/23 [00:00<00:00, 25.19it/s]\n","epoch 447 iter 22: train loss 1.30335. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 24.81it/s]\n","epoch 448 iter 22: train loss 1.22073. lr 1.265976e-03: 100% 23/23 [00:00<00:00, 24.99it/s]\n","epoch 449 iter 22: train loss 1.35325. lr 5.464011e-03: 100% 23/23 [00:00<00:00, 25.43it/s]\n","epoch 450 iter 22: train loss 1.41081. lr 4.688490e-03: 100% 23/23 [00:00<00:00, 25.18it/s]\n","epoch 451 iter 22: train loss 1.28090. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 24.93it/s]\n","epoch 452 iter 22: train loss 1.22059. lr 1.357620e-03: 100% 23/23 [00:00<00:00, 25.06it/s]\n","epoch 453 iter 22: train loss 1.29296. lr 5.525563e-03: 100% 23/23 [00:00<00:00, 25.55it/s]\n","epoch 454 iter 22: train loss 1.40330. lr 4.595709e-03: 100% 23/23 [00:00<00:00, 24.62it/s]\n","epoch 455 iter 22: train loss 1.27160. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 19.13it/s]\n","epoch 456 iter 22: train loss 1.22173. lr 1.451507e-03: 100% 23/23 [00:01<00:00, 18.75it/s]\n","epoch 457 iter 22: train loss 1.28354. lr 5.583667e-03: 100% 23/23 [00:01<00:00, 18.37it/s]\n","epoch 458 iter 22: train loss 1.38289. lr 4.500748e-03: 100% 23/23 [00:01<00:00, 18.38it/s]\n","epoch 459 iter 22: train loss 1.24556. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 21.02it/s]\n","epoch 460 iter 22: train loss 1.16816. lr 1.547509e-03: 100% 23/23 [00:00<00:00, 24.38it/s]\n","epoch 461 iter 22: train loss 1.29247. lr 5.638241e-03: 100% 23/23 [00:00<00:00, 24.76it/s]\n","epoch 462 iter 22: train loss 1.39064. lr 4.403737e-03: 100% 23/23 [00:00<00:00, 25.04it/s]\n","epoch 463 iter 22: train loss 1.31955. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 24.68it/s]\n","epoch 464 iter 22: train loss 1.26192. lr 1.645495e-03: 100% 23/23 [00:00<00:00, 25.41it/s]\n","epoch 465 iter 22: train loss 1.29059. lr 5.689212e-03: 100% 23/23 [00:00<00:00, 25.12it/s]\n","epoch 466 iter 22: train loss 1.32835. lr 4.304809e-03: 100% 23/23 [00:01<00:00, 19.51it/s]\n","epoch 467 iter 22: train loss 1.21701. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 23.69it/s]\n","epoch 468 iter 22: train loss 1.19258. lr 1.745332e-03: 100% 23/23 [00:01<00:00, 21.26it/s]\n","epoch 469 iter 22: train loss 1.28568. lr 5.736510e-03: 100% 23/23 [00:01<00:00, 18.24it/s]\n","epoch 470 iter 22: train loss 1.33847. lr 4.204099e-03: 100% 23/23 [00:01<00:00, 18.45it/s]\n","epoch 471 iter 22: train loss 1.26549. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.36it/s]\n","epoch 472 iter 22: train loss 1.21267. lr 1.846881e-03: 100% 23/23 [00:01<00:00, 20.37it/s]\n","epoch 473 iter 22: train loss 1.30533. lr 5.780070e-03: 100% 23/23 [00:00<00:00, 25.16it/s]\n","epoch 474 iter 22: train loss 1.38451. lr 4.101744e-03: 100% 23/23 [00:00<00:00, 25.31it/s]\n","epoch 475 iter 22: train loss 1.19882. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 25.07it/s]\n","epoch 476 iter 22: train loss 1.21884. lr 1.950006e-03: 100% 23/23 [00:00<00:00, 24.85it/s]\n","epoch 477 iter 22: train loss 1.33594. lr 5.819833e-03: 100% 23/23 [00:00<00:00, 24.97it/s]\n","epoch 478 iter 22: train loss 1.37906. lr 3.997885e-03: 100% 23/23 [00:00<00:00, 24.82it/s]\n","epoch 479 iter 22: train loss 1.25171. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 25.56it/s]\n","epoch 480 iter 22: train loss 1.18376. lr 2.054565e-03: 100% 23/23 [00:00<00:00, 24.66it/s]\n","epoch 481 iter 22: train loss 1.31781. lr 5.855745e-03: 100% 23/23 [00:00<00:00, 24.27it/s]\n","epoch 482 iter 22: train loss 1.30183. lr 3.892662e-03: 100% 23/23 [00:01<00:00, 18.36it/s]\n","epoch 483 iter 22: train loss 1.25038. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 19.15it/s]\n","epoch 484 iter 22: train loss 1.19755. lr 2.160415e-03: 100% 23/23 [00:01<00:00, 18.66it/s]\n","epoch 485 iter 22: train loss 1.26743. lr 5.887756e-03: 100% 23/23 [00:01<00:00, 18.76it/s]\n","epoch 486 iter 22: train loss 1.31138. lr 3.786220e-03: 100% 23/23 [00:01<00:00, 21.47it/s]\n","epoch 487 iter 22: train loss 1.21210. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 25.54it/s]\n","epoch 488 iter 22: train loss 1.20404. lr 2.267412e-03: 100% 23/23 [00:00<00:00, 25.10it/s]\n","epoch 489 iter 22: train loss 1.33185. lr 5.915823e-03: 100% 23/23 [00:00<00:00, 25.44it/s]\n","epoch 490 iter 22: train loss 1.38838. lr 3.678705e-03: 100% 23/23 [00:00<00:00, 25.26it/s]\n","epoch 491 iter 22: train loss 1.26054. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 24.52it/s]\n","epoch 492 iter 22: train loss 1.17408. lr 2.375410e-03: 100% 23/23 [00:01<00:00, 22.20it/s]\n","epoch 493 iter 22: train loss 1.35394. lr 5.939907e-03: 100% 23/23 [00:00<00:00, 25.26it/s]\n","epoch 494 iter 22: train loss 1.32061. lr 3.570262e-03: 100% 23/23 [00:00<00:00, 24.39it/s]\n","epoch 495 iter 22: train loss 1.27693. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 21.73it/s]\n","epoch 496 iter 22: train loss 1.20242. lr 2.484261e-03: 100% 23/23 [00:01<00:00, 18.56it/s]\n","epoch 497 iter 22: train loss 1.28992. lr 5.959976e-03: 100% 23/23 [00:01<00:00, 19.13it/s]\n","epoch 498 iter 22: train loss 1.28424. lr 3.461040e-03: 100% 23/23 [00:01<00:00, 18.35it/s]\n","epoch 499 iter 22: train loss 1.19541. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.88it/s]\n","epoch 500 iter 22: train loss 1.19514. lr 2.593816e-03: 100% 23/23 [00:00<00:00, 25.31it/s]\n","epoch 501 iter 22: train loss 1.32349. lr 5.976001e-03: 100% 23/23 [00:00<00:00, 25.64it/s]\n","epoch 502 iter 22: train loss 1.32694. lr 3.351189e-03: 100% 23/23 [00:00<00:00, 24.72it/s]\n","epoch 503 iter 22: train loss 1.21377. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 25.15it/s]\n","epoch 504 iter 22: train loss 1.16749. lr 2.703926e-03: 100% 23/23 [00:00<00:00, 25.02it/s]\n","epoch 505 iter 22: train loss 1.32256. lr 5.987962e-03: 100% 23/23 [00:00<00:00, 24.90it/s]\n","epoch 506 iter 22: train loss 1.35636. lr 3.240858e-03: 100% 23/23 [00:00<00:00, 24.92it/s]\n","epoch 507 iter 22: train loss 1.22480. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 24.94it/s]\n","epoch 508 iter 22: train loss 1.16059. lr 2.814440e-03: 100% 23/23 [00:01<00:00, 23.00it/s]\n","epoch 509 iter 22: train loss 1.31149. lr 5.995842e-03: 100% 23/23 [00:01<00:00, 19.30it/s]\n","epoch 510 iter 22: train loss 1.33456. lr 3.130198e-03: 100% 23/23 [00:01<00:00, 19.35it/s]\n","epoch 511 iter 22: train loss 1.20562. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.16it/s]\n","epoch 512 iter 22: train loss 1.17107. lr 2.925208e-03: 100% 23/23 [00:01<00:00, 16.90it/s]\n","epoch 513 iter 22: train loss 1.22925. lr 5.999631e-03: 100% 23/23 [00:00<00:00, 23.01it/s]\n","epoch 514 iter 22: train loss 1.29519. lr 3.019360e-03: 100% 23/23 [00:00<00:00, 24.29it/s]\n","epoch 515 iter 22: train loss 1.15018. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 24.39it/s]\n","epoch 516 iter 22: train loss 1.14912. lr 3.036078e-03: 100% 23/23 [00:00<00:00, 25.31it/s]\n","epoch 517 iter 22: train loss 1.35252. lr 5.999322e-03: 100% 23/23 [00:00<00:00, 24.43it/s]\n","epoch 518 iter 22: train loss 1.31844. lr 2.908496e-03: 100% 23/23 [00:01<00:00, 19.54it/s]\n","epoch 519 iter 22: train loss 1.16555. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 24.91it/s]\n","epoch 520 iter 22: train loss 1.17172. lr 3.146898e-03: 100% 23/23 [00:00<00:00, 25.44it/s]\n","epoch 521 iter 22: train loss 1.32163. lr 5.994916e-03: 100% 23/23 [00:00<00:00, 24.97it/s]\n","epoch 522 iter 22: train loss 1.28420. lr 2.797757e-03: 100% 23/23 [00:01<00:00, 18.75it/s]\n","epoch 523 iter 22: train loss 1.14377. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.92it/s]\n","epoch 524 iter 22: train loss 1.15334. lr 3.257519e-03: 100% 23/23 [00:01<00:00, 18.59it/s]\n","epoch 525 iter 22: train loss 1.28168. lr 5.986420e-03: 100% 23/23 [00:01<00:00, 18.08it/s]\n","epoch 526 iter 22: train loss 1.29150. lr 2.687294e-03: 100% 23/23 [00:00<00:00, 24.44it/s]\n","epoch 527 iter 22: train loss 1.19623. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 24.92it/s]\n","epoch 528 iter 22: train loss 1.22680. lr 3.367787e-03: 100% 23/23 [00:00<00:00, 25.31it/s]\n","epoch 529 iter 22: train loss 1.31340. lr 5.973845e-03: 100% 23/23 [00:00<00:00, 23.98it/s]\n","epoch 530 iter 22: train loss 1.27940. lr 2.577258e-03: 100% 23/23 [00:00<00:00, 25.94it/s]\n","epoch 531 iter 22: train loss 1.18907. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 24.94it/s]\n","epoch 532 iter 22: train loss 1.23856. lr 3.477553e-03: 100% 23/23 [00:00<00:00, 24.28it/s]\n","epoch 533 iter 22: train loss 1.31498. lr 5.957208e-03: 100% 23/23 [00:00<00:00, 25.02it/s]\n","epoch 534 iter 22: train loss 1.26182. lr 2.467800e-03: 100% 23/23 [00:00<00:00, 24.23it/s]\n","epoch 535 iter 22: train loss 1.21503. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.60it/s]\n","epoch 536 iter 22: train loss 1.14202. lr 3.586666e-03: 100% 23/23 [00:01<00:00, 18.73it/s]\n","epoch 537 iter 22: train loss 1.35160. lr 5.936532e-03: 100% 23/23 [00:01<00:00, 18.65it/s]\n","epoch 538 iter 22: train loss 1.25091. lr 2.359068e-03: 100% 23/23 [00:01<00:00, 18.53it/s]\n","epoch 539 iter 22: train loss 1.12508. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 20.57it/s]\n","epoch 540 iter 22: train loss 1.09140. lr 3.694979e-03: 100% 23/23 [00:00<00:00, 24.81it/s]\n","epoch 541 iter 22: train loss 1.30360. lr 5.911845e-03: 100% 23/23 [00:00<00:00, 25.01it/s]\n","epoch 542 iter 22: train loss 1.28805. lr 2.251212e-03: 100% 23/23 [00:00<00:00, 23.72it/s]\n","epoch 543 iter 22: train loss 1.19666. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 21.01it/s]\n","epoch 544 iter 22: train loss 1.14804. lr 3.802342e-03: 100% 23/23 [00:00<00:00, 25.09it/s]\n","epoch 545 iter 22: train loss 1.29200. lr 5.883181e-03: 100% 23/23 [00:00<00:00, 24.87it/s]\n","epoch 546 iter 22: train loss 1.24239. lr 2.144378e-03: 100% 23/23 [00:00<00:00, 24.36it/s]\n","epoch 547 iter 22: train loss 1.14040. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 25.33it/s]\n","epoch 548 iter 22: train loss 1.17250. lr 3.908609e-03: 100% 23/23 [00:01<00:00, 21.85it/s]\n","epoch 549 iter 22: train loss 1.28602. lr 5.850579e-03: 100% 23/23 [00:01<00:00, 18.40it/s]\n","epoch 550 iter 22: train loss 1.23757. lr 2.038714e-03: 100% 23/23 [00:01<00:00, 18.70it/s]\n","epoch 551 iter 22: train loss 1.13427. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.81it/s]\n","epoch 552 iter 22: train loss 1.16797. lr 4.013635e-03: 100% 23/23 [00:01<00:00, 18.99it/s]\n","epoch 553 iter 22: train loss 1.30457. lr 5.814083e-03: 100% 23/23 [00:00<00:00, 25.48it/s]\n","epoch 554 iter 22: train loss 1.27118. lr 1.934362e-03: 100% 23/23 [00:00<00:00, 25.19it/s]\n","epoch 555 iter 22: train loss 1.18011. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 25.10it/s]\n","epoch 556 iter 22: train loss 1.17277. lr 4.117277e-03: 100% 23/23 [00:00<00:00, 25.33it/s]\n","epoch 557 iter 22: train loss 1.31588. lr 5.773744e-03: 100% 23/23 [00:00<00:00, 25.25it/s]\n","epoch 558 iter 22: train loss 1.28073. lr 1.831465e-03: 100% 23/23 [00:00<00:00, 24.49it/s]\n","epoch 559 iter 22: train loss 1.14792. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 24.86it/s]\n","epoch 560 iter 22: train loss 1.17954. lr 4.219393e-03: 100% 23/23 [00:00<00:00, 25.50it/s]\n","epoch 561 iter 22: train loss 1.26860. lr 5.729616e-03: 100% 23/23 [00:00<00:00, 24.68it/s]\n","epoch 562 iter 22: train loss 1.23662. lr 1.730165e-03: 100% 23/23 [00:01<00:00, 18.84it/s]\n","epoch 563 iter 22: train loss 1.12943. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.89it/s]\n","epoch 564 iter 22: train loss 1.14556. lr 4.319843e-03: 100% 23/23 [00:01<00:00, 19.20it/s]\n","epoch 565 iter 22: train loss 1.30750. lr 5.681760e-03: 100% 23/23 [00:01<00:00, 19.34it/s]\n","epoch 566 iter 22: train loss 1.25806. lr 1.630599e-03: 100% 23/23 [00:01<00:00, 20.09it/s]\n","epoch 567 iter 22: train loss 1.12769. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 25.21it/s]\n","epoch 568 iter 22: train loss 1.20180. lr 4.418491e-03: 100% 23/23 [00:00<00:00, 25.02it/s]\n","epoch 569 iter 22: train loss 1.28088. lr 5.630241e-03: 100% 23/23 [00:01<00:00, 20.34it/s]\n","epoch 570 iter 22: train loss 1.14645. lr 1.532904e-03: 100% 23/23 [00:00<00:00, 24.31it/s]\n","epoch 571 iter 22: train loss 1.15506. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 25.08it/s]\n","epoch 572 iter 22: train loss 1.17510. lr 4.515201e-03: 100% 23/23 [00:00<00:00, 25.21it/s]\n","epoch 573 iter 22: train loss 1.26117. lr 5.575130e-03: 100% 23/23 [00:00<00:00, 24.43it/s]\n","epoch 574 iter 22: train loss 1.20682. lr 1.437212e-03: 100% 23/23 [00:00<00:00, 25.17it/s]\n","epoch 575 iter 22: train loss 1.11205. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 21.48it/s]\n","epoch 576 iter 22: train loss 1.14518. lr 4.609841e-03: 100% 23/23 [00:01<00:00, 18.61it/s]\n","epoch 577 iter 22: train loss 1.29481. lr 5.516501e-03: 100% 23/23 [00:01<00:00, 18.68it/s]\n","epoch 578 iter 22: train loss 1.20333. lr 1.343655e-03: 100% 23/23 [00:01<00:00, 18.36it/s]\n","epoch 579 iter 22: train loss 1.12363. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.55it/s]\n","epoch 580 iter 22: train loss 1.17096. lr 4.702283e-03: 100% 23/23 [00:00<00:00, 24.47it/s]\n","epoch 581 iter 22: train loss 1.29966. lr 5.454436e-03: 100% 23/23 [00:00<00:00, 25.74it/s]\n","epoch 582 iter 22: train loss 1.15224. lr 1.252360e-03: 100% 23/23 [00:00<00:00, 25.02it/s]\n","epoch 583 iter 22: train loss 1.09768. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 24.58it/s]\n","epoch 584 iter 22: train loss 1.19932. lr 4.792400e-03: 100% 23/23 [00:00<00:00, 25.40it/s]\n","epoch 585 iter 22: train loss 1.27714. lr 5.389018e-03: 100% 23/23 [00:00<00:00, 24.20it/s]\n","epoch 586 iter 22: train loss 1.25687. lr 1.163453e-03: 100% 23/23 [00:00<00:00, 24.91it/s]\n","epoch 587 iter 22: train loss 1.13826. lr 6.449209e-04: 100% 23/23 [00:00<00:00, 24.42it/s]\n","epoch 588 iter 22: train loss 1.17917. lr 4.880068e-03: 100% 23/23 [00:00<00:00, 25.30it/s]\n","epoch 589 iter 22: train loss 1.33675. lr 5.320336e-03: 100% 23/23 [00:01<00:00, 18.20it/s]\n","epoch 590 iter 22: train loss 1.18801. lr 1.077053e-03: 100% 23/23 [00:01<00:00, 17.76it/s]\n","epoch 591 iter 22: train loss 1.18552. lr 7.151987e-04: 100% 23/23 [00:01<00:00, 17.72it/s]\n","epoch 592 iter 22: train loss 1.24150. lr 4.965169e-03: 100% 23/23 [00:01<00:00, 19.03it/s]\n","epoch 593 iter 22: train loss 1.28200. lr 5.248486e-03: 100% 23/23 [00:01<00:00, 19.43it/s]\n","epoch 594 iter 22: train loss 1.15935. lr 9.932805e-04: 100% 23/23 [00:01<00:00, 21.06it/s]\n","epoch 595 iter 22: train loss 1.12736. lr 7.885972e-04: 100% 23/23 [00:00<00:00, 24.69it/s]\n","epoch 596 iter 22: train loss 1.16824. lr 5.047585e-03: 100% 23/23 [00:00<00:00, 24.53it/s]\n","epoch 597 iter 22: train loss 1.29186. lr 5.173564e-03: 100% 23/23 [00:00<00:00, 24.24it/s]\n","epoch 598 iter 22: train loss 1.14702. lr 9.122485e-04: 100% 23/23 [00:00<00:00, 25.16it/s]\n","epoch 599 iter 22: train loss 1.11127. lr 8.650161e-04: 100% 23/23 [00:00<00:00, 24.80it/s]\n","epoch 600 iter 22: train loss 1.22915. lr 5.127205e-03: 100% 23/23 [00:00<00:00, 24.49it/s]\n","epoch 601 iter 22: train loss 1.31467. lr 5.095674e-03: 100% 23/23 [00:00<00:00, 25.01it/s]\n","epoch 602 iter 22: train loss 1.22142. lr 8.340680e-04: 100% 23/23 [00:01<00:00, 19.18it/s]\n","epoch 603 iter 22: train loss 1.10513. lr 9.443511e-04: 100% 23/23 [00:01<00:00, 16.03it/s]\n","epoch 604 iter 22: train loss 1.16673. lr 5.203919e-03: 100% 23/23 [00:01<00:00, 18.08it/s]\n","epoch 605 iter 22: train loss 1.24112. lr 5.014922e-03: 100% 23/23 [00:01<00:00, 18.61it/s]\n","epoch 606 iter 22: train loss 1.18195. lr 7.588460e-04: 100% 23/23 [00:01<00:00, 20.13it/s]\n","epoch 607 iter 22: train loss 1.17927. lr 1.026494e-03: 100% 23/23 [00:00<00:00, 24.80it/s]\n","epoch 608 iter 22: train loss 1.23059. lr 5.277623e-03: 100% 23/23 [00:00<00:00, 24.24it/s]\n","epoch 609 iter 22: train loss 1.29153. lr 4.931417e-03: 100% 23/23 [00:00<00:00, 24.87it/s]\n","epoch 610 iter 22: train loss 1.16031. lr 6.866850e-04: 100% 23/23 [00:00<00:00, 23.60it/s]\n","epoch 611 iter 22: train loss 1.12933. lr 1.111332e-03: 100% 23/23 [00:00<00:00, 24.07it/s]\n","epoch 612 iter 22: train loss 1.20892. lr 5.348217e-03: 100% 23/23 [00:00<00:00, 25.16it/s]\n","epoch 613 iter 22: train loss 1.21461. lr 4.845274e-03: 100% 23/23 [00:00<00:00, 23.57it/s]\n","epoch 614 iter 22: train loss 1.21765. lr 6.176836e-04: 100% 23/23 [00:00<00:00, 24.30it/s]\n","epoch 615 iter 22: train loss 1.07968. lr 1.198750e-03: 100% 23/23 [00:01<00:00, 22.22it/s]\n","epoch 616 iter 22: train loss 1.17030. lr 5.415603e-03: 100% 23/23 [00:01<00:00, 18.84it/s]\n","epoch 617 iter 22: train loss 1.28929. lr 4.756611e-03: 100% 23/23 [00:01<00:00, 18.29it/s]\n","epoch 618 iter 22: train loss 1.16896. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 18.58it/s]\n","epoch 619 iter 22: train loss 1.10659. lr 1.288628e-03: 100% 23/23 [00:01<00:00, 18.47it/s]\n","epoch 620 iter 22: train loss 1.13457. lr 5.479689e-03: 100% 23/23 [00:01<00:00, 22.42it/s]\n","epoch 621 iter 22: train loss 1.22784. lr 4.665549e-03: 100% 23/23 [00:01<00:00, 20.72it/s]\n","epoch 622 iter 22: train loss 1.13105. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 24.88it/s]\n","epoch 623 iter 22: train loss 1.16836. lr 1.380844e-03: 100% 23/23 [00:00<00:00, 24.24it/s]\n","epoch 624 iter 22: train loss 1.20751. lr 5.540389e-03: 100% 23/23 [00:00<00:00, 24.84it/s]\n","epoch 625 iter 22: train loss 1.27620. lr 4.572211e-03: 100% 23/23 [00:00<00:00, 24.96it/s]\n","epoch 626 iter 22: train loss 1.17758. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 24.58it/s]\n","epoch 627 iter 22: train loss 1.17571. lr 1.475271e-03: 100% 23/23 [00:00<00:00, 24.61it/s]\n","epoch 628 iter 22: train loss 1.19526. lr 5.597619e-03: 100% 23/23 [00:00<00:00, 23.65it/s]\n","epoch 629 iter 22: train loss 1.21185. lr 4.476727e-03: 100% 23/23 [00:01<00:00, 17.87it/s]\n","epoch 630 iter 22: train loss 1.17810. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 19.49it/s]\n","epoch 631 iter 22: train loss 1.10052. lr 1.571780e-03: 100% 23/23 [00:01<00:00, 18.44it/s]\n","epoch 632 iter 22: train loss 1.22963. lr 5.651301e-03: 100% 23/23 [00:01<00:00, 18.75it/s]\n","epoch 633 iter 22: train loss 1.30055. lr 4.379225e-03: 100% 23/23 [00:00<00:00, 23.16it/s]\n","epoch 634 iter 22: train loss 1.12071. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 24.40it/s]\n","epoch 635 iter 22: train loss 1.01370. lr 1.670241e-03: 100% 23/23 [00:00<00:00, 24.84it/s]\n","epoch 636 iter 22: train loss 1.20649. lr 5.701361e-03: 100% 23/23 [00:00<00:00, 24.84it/s]\n","epoch 637 iter 22: train loss 1.22677. lr 4.279840e-03: 100% 23/23 [00:00<00:00, 23.44it/s]\n","epoch 638 iter 22: train loss 1.14955. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 24.86it/s]\n","epoch 639 iter 22: train loss 1.11128. lr 1.770517e-03: 100% 23/23 [00:00<00:00, 24.38it/s]\n","epoch 640 iter 22: train loss 1.16995. lr 5.747732e-03: 100% 23/23 [00:00<00:00, 25.28it/s]\n","epoch 641 iter 22: train loss 1.23862. lr 4.178706e-03: 100% 23/23 [00:00<00:00, 25.46it/s]\n","epoch 642 iter 22: train loss 1.19710. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 20.50it/s]\n","epoch 643 iter 22: train loss 1.12872. lr 1.872473e-03: 100% 23/23 [00:01<00:00, 18.76it/s]\n","epoch 644 iter 22: train loss 1.20862. lr 5.790350e-03: 100% 23/23 [00:01<00:00, 19.22it/s]\n","epoch 645 iter 22: train loss 1.20401. lr 4.075962e-03: 100% 23/23 [00:01<00:00, 17.59it/s]\n","epoch 646 iter 22: train loss 1.16811. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 20.14it/s]\n","epoch 647 iter 22: train loss 1.06662. lr 1.975969e-03: 100% 23/23 [00:01<00:00, 23.00it/s]\n","epoch 648 iter 22: train loss 1.18774. lr 5.829157e-03: 100% 23/23 [00:00<00:00, 24.44it/s]\n","epoch 649 iter 22: train loss 1.24534. lr 3.971749e-03: 100% 23/23 [00:00<00:00, 25.16it/s]\n","epoch 650 iter 22: train loss 1.19087. lr 6.000000e-04: 100% 23/23 [00:00<00:00, 25.06it/s]\n","2023-12-19 17:29:06.299012: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2023-12-19 17:29:06.299065: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2023-12-19 17:29:06.300459: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2023-12-19 17:29:06.308242: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-12-19 17:29:07.415644: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","data has 418352 characters, 256 unique.\n","number of parameters: 3339776\n","epoch 1 iter 7: train loss 1.12608. lr 5.999844e-04: 100% 8/8 [00:02<00:00,  3.81it/s]\n","epoch 2 iter 7: train loss 0.93858. lr 5.999351e-04: 100% 8/8 [00:00<00:00, 12.24it/s]\n","epoch 3 iter 7: train loss 0.84357. lr 5.998521e-04: 100% 8/8 [00:00<00:00, 14.05it/s]\n","epoch 4 iter 7: train loss 0.79899. lr 5.997352e-04: 100% 8/8 [00:00<00:00, 13.92it/s]\n","epoch 5 iter 7: train loss 0.75244. lr 5.995847e-04: 100% 8/8 [00:00<00:00, 13.73it/s]\n","epoch 6 iter 7: train loss 0.71741. lr 5.994004e-04: 100% 8/8 [00:00<00:00, 14.23it/s]\n","epoch 7 iter 7: train loss 0.67602. lr 5.991823e-04: 100% 8/8 [00:00<00:00, 14.17it/s]\n","epoch 8 iter 7: train loss 0.64391. lr 5.989306e-04: 100% 8/8 [00:00<00:00, 13.72it/s]\n","epoch 9 iter 7: train loss 0.61117. lr 5.986453e-04: 100% 8/8 [00:00<00:00, 13.99it/s]\n","epoch 10 iter 7: train loss 0.57356. lr 5.983263e-04: 100% 8/8 [00:00<00:00, 13.99it/s]\n","2023-12-19 17:29:23.204353: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2023-12-19 17:29:23.204415: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2023-12-19 17:29:23.205673: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2023-12-19 17:29:23.212894: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-12-19 17:29:24.529391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","data has 418352 characters, 256 unique.\n","number of parameters: 3339776\n","500it [00:57,  8.75it/s]\n","Correct: 20.0 out of 500.0: 4.0%\n","2023-12-19 17:30:27.829240: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2023-12-19 17:30:27.829301: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2023-12-19 17:30:27.830662: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2023-12-19 17:30:27.838164: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-12-19 17:30:28.935982: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","data has 418352 characters, 256 unique.\n","number of parameters: 3339776\n","437it [00:49,  8.91it/s]\n","No gold birth places provided; returning (0,0)\n","Predictions written to perceiver.pretrain.test.predictions; no targets provided\n"]}]},{"cell_type":"markdown","source":["#### After you have verified that everything works, you can collect your submission\n"],"metadata":{"id":"fy9Ny98mz-Kw"}},{"cell_type":"code","source":["! sh collect_submission.sh"],"metadata":{"id":"7oPYwSDVAI29"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"KD9B9IbBrPRK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 5. Download your files\n","If you used Option 2, where your files are not directly in your Google drive, you would need to download the files you have generated in the process. You can zip the entire folder and download the whole folder in the following cell block:"],"metadata":{"id":"TiwHMGNlFhK_"}},{"cell_type":"code","source":["!zip folder_name downloading.zip\n","from google.colab import files\n","files.download('downloading.zip')"],"metadata":{"id":"QIO3ftjEFxDX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"2tkEd8Gq7akI"},"execution_count":null,"outputs":[]}]}